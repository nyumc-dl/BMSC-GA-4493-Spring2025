{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Introduction to Convolutional Layers\n",
    "\n",
    "The goal of this lab is to implement convolutional layers in pytorch and illustrate how to use GPU for training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import common dependencies\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question: what would be the desired input shape for the 2D Convolution layer?\n",
    "desired_shape = (3,15,15) # how does this relate to the input channels?\n",
    "\n",
    "# generate a random tensor with the desired shape\n",
    "x_2d = torch.rand(desired_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowing the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0min_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkernel_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdilation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgroups\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Applies a 2D convolution over an input signal composed of several input\n",
      "planes.\n",
      "\n",
      "In the simplest case, the output value of the layer with input size\n",
      ":math:`(N, C_{\\text{in}}, H, W)` and output :math:`(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})`\n",
      "can be precisely described as:\n",
      "\n",
      ".. math::\n",
      "    \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n",
      "    \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\n",
      "\n",
      "\n",
      "where :math:`\\star` is the valid 2D `cross-correlation`_ operator,\n",
      ":math:`N` is a batch size, :math:`C` denotes a number of channels,\n",
      ":math:`H` is a height of input planes in pixels, and :math:`W` is\n",
      "width in pixels.\n",
      "\n",
      "\n",
      "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "* :attr:`stride` controls the stride for the cross-correlation, a single\n",
      "  number or a tuple.\n",
      "\n",
      "* :attr:`padding` controls the amount of padding applied to the input. It\n",
      "  can be either a string {'valid', 'same'} or an int / a tuple of ints giving the\n",
      "  amount of implicit padding applied on both sides.\n",
      "\n",
      "* :attr:`dilation` controls the spacing between the kernel points; also\n",
      "  known as the Ã  trous algorithm. It is harder to describe, but this `link`_\n",
      "  has a nice visualization of what :attr:`dilation` does.\n",
      "\n",
      "\n",
      "* :attr:`groups` controls the connections between inputs and outputs.\n",
      "  :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n",
      "  :attr:`groups`. For example,\n",
      "\n",
      "    * At groups=1, all inputs are convolved to all outputs.\n",
      "    * At groups=2, the operation becomes equivalent to having two conv\n",
      "      layers side by side, each seeing half the input channels\n",
      "      and producing half the output channels, and both subsequently\n",
      "      concatenated.\n",
      "    * At groups= :attr:`in_channels`, each input channel is convolved with\n",
      "      its own set of filters (of size\n",
      "      :math:`\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}`).\n",
      "\n",
      "The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
      "\n",
      "    - a single ``int`` -- in which case the same value is used for the height and width dimension\n",
      "    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n",
      "      and the second `int` for the width dimension\n",
      "\n",
      "Note:\n",
      "    When `groups == in_channels` and `out_channels == K * in_channels`,\n",
      "    where `K` is a positive integer, this operation is also known as a \"depthwise convolution\".\n",
      "\n",
      "    In other words, for an input of size :math:`(N, C_{in}, L_{in})`,\n",
      "    a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments\n",
      "    :math:`(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in})`.\n",
      "\n",
      "Note:\n",
      "    In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
      "\n",
      "Note:\n",
      "    ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
      "    the input so the output has the shape as the input. However, this mode\n",
      "    doesn't support any stride values other than 1.\n",
      "\n",
      "Note:\n",
      "    This module supports complex data types i.e. ``complex32, complex64, complex128``.\n",
      "\n",
      "Args:\n",
      "    in_channels (int): Number of channels in the input image\n",
      "    out_channels (int): Number of channels produced by the convolution\n",
      "    kernel_size (int or tuple): Size of the convolving kernel\n",
      "    stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
      "    padding (int, tuple or str, optional): Padding added to all four sides of\n",
      "        the input. Default: 0\n",
      "    dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
      "    groups (int, optional): Number of blocked connections from input\n",
      "        channels to output channels. Default: 1\n",
      "    bias (bool, optional): If ``True``, adds a learnable bias to the\n",
      "        output. Default: ``True``\n",
      "    padding_mode (str, optional): ``'zeros'``, ``'reflect'``,\n",
      "        ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
      "\n",
      "\n",
      "Shape:\n",
      "    - Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`\n",
      "    - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where\n",
      "\n",
      "      .. math::\n",
      "          H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
      "                    \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
      "\n",
      "      .. math::\n",
      "          W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
      "                    \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
      "\n",
      "Attributes:\n",
      "    weight (Tensor): the learnable weights of the module of shape\n",
      "        :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\n",
      "        :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\n",
      "        The values of these weights are sampled from\n",
      "        :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "        :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
      "    bias (Tensor):   the learnable bias of the module of shape\n",
      "        (out_channels). If :attr:`bias` is ``True``,\n",
      "        then the values of these weights are\n",
      "        sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "        :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
      "\n",
      "Examples:\n",
      "\n",
      "    >>> # With square kernels and equal stride\n",
      "    >>> m = nn.Conv2d(16, 33, 3, stride=2)\n",
      "    >>> # non-square kernels and unequal stride and with padding\n",
      "    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
      "    >>> # non-square kernels and unequal stride and with padding and dilation\n",
      "    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
      "    >>> input = torch.randn(20, 16, 50, 100)\n",
      "    >>> output = m(input)\n",
      "\n",
      ".. _cross-correlation:\n",
      "    https://en.wikipedia.org/wiki/Cross-correlation\n",
      "\n",
      ".. _link:\n",
      "    https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           /scratch/ca3261/dl/lib/python3.9/site-packages/torch/nn/modules/conv.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     LazyConv2d, Conv2d, ConvBn2d, Conv2d"
     ]
    }
   ],
   "source": [
    "?nn.Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create a Convolution layer and see how that works, let's understand what parameters does our 2D convolution layer need and what would be the expected output?\n",
    "\n",
    "### Questions\n",
    "- Input channels?\n",
    "    - Number of slices of 2d matrices\n",
    "- Output channels?\n",
    "    - Number of filters. Each filter will result in a slice of matrix by being applied to the input. One filter will be operating on all channels of the input, so the filter shape will be (in_channels, kernel_size, kernel_size).\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a 2D Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how many in channels do we have? (check our sample tensor)\n",
    "conv2d_layer = nn.Conv2d(in_channels=3,  # input channels\n",
    "                         out_channels=5, # output channels\n",
    "                         kernel_size=3, # kernel size\n",
    "                         stride=1, # stride\n",
    "                         padding=1, # padding\n",
    "                         bias=True\n",
    "                        ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Parameters in the Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in our conv2d_layer are 140\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of parameters in our conv2d_layer are {sum(p.numel() for p in conv2d_layer.parameters())}')\n",
    "# (filter size + 1 for bias) x output channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 15, 15])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 15, 15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2d.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv2d_layer(x_2d.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of the output??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 15, 15])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output shape of torch.Size([1, 8, 5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 5, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: create a 2D convolution layer that has an output shape of torch.Size([1, 8, 5, 5]) with the same input tensor\n",
    "conv2d_layer_2 = nn.Conv2d(in_channels=3,  # input channels\n",
    "                         out_channels=8, # output channels\n",
    "                         kernel_size=5, # kernel size\n",
    "                         stride=3, # stride\n",
    "                         padding=1, # padding\n",
    "                         bias=True\n",
    "                        ) \n",
    "\n",
    "out = conv2d_layer_2(x_2d.unsqueeze(0))\n",
    "out.shape\n",
    "# NOTE: ther are multiple ways to do this so there is not one correct answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look of 1d convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question: What would be the desired input shape for the 1D Convolution layer?\n",
    "desired_shape = (3,10)\n",
    "\n",
    "# generate a random tensor of the desired shape\n",
    "x_1d = torch.rand(desired_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a 1D Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d_layer = nn.Conv1d(in_channels=3,  # input channels\n",
    "                         out_channels=5, # output channels\n",
    "                         kernel_size=3, # kernel size\n",
    "                         stride=1, # stride\n",
    "                         padding=1, # padding\n",
    "                         bias=True\n",
    "                        ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Parameters in the Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in our conv1d_layer are 50\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of parameters in our conv1d_layer are {sum(p.numel() for p in conv1d_layer.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d_res = conv1d_layer(x_1d.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # input channels, output channels, kernel size\n",
    "        self.pool = nn.MaxPool2d(2, 2) # kernel size, stride\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120) # 16*4*4 comes from the dimensionality of the output tensor before the fully connected layers\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10) # 10 output classes for MNIST digits\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4) # flatten the tensor for the fully connected layer\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform = transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Training Epoch: 1 - Time: 8.15 seconds\n",
      "Training Epoch: 2 - Time: 5.90 seconds\n",
      "Training Epoch: 3 - Time: 5.94 seconds\n",
      "Training Epoch: 4 - Time: 5.91 seconds\n",
      "Training Epoch: 5 - Time: 5.88 seconds\n",
      "Training Epoch: 6 - Time: 5.91 seconds\n",
      "Training Epoch: 7 - Time: 5.90 seconds\n",
      "Training Epoch: 8 - Time: 5.89 seconds\n",
      "Training Epoch: 9 - Time: 5.91 seconds\n",
      "Training Epoch: 10 - Time: 5.90 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Using', device)\n",
    "model = LeNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Training Epoch: {} - Time: {:.2f} seconds'.format(epoch, time.time() - start_time))\n",
    "\n",
    "for epoch in range(1, 11): # Train for 10 epochs for demonstration\n",
    "    train(model, device, train_loader, optimizer, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0001, Accuracy: 9781/10000 (98%)\n"
     ]
    }
   ],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Your Own CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: see if you can achieve a similar accuracy to LeNet with your own CNN\n",
    "# make sure to have a different number of convolutional layers in your appraoch\n",
    "\n",
    "class CassieNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CassieNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, 1, 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 10, 2)\n",
    "        self.fc1 = nn.Linear(10, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 10)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Training Epoch: 1 - Time: 6.15 seconds\n",
      "Training Epoch: 2 - Time: 6.17 seconds\n",
      "Training Epoch: 3 - Time: 6.14 seconds\n",
      "Training Epoch: 4 - Time: 6.15 seconds\n",
      "Training Epoch: 5 - Time: 6.14 seconds\n",
      "Training Epoch: 6 - Time: 6.16 seconds\n",
      "Training Epoch: 7 - Time: 6.14 seconds\n",
      "Training Epoch: 8 - Time: 6.14 seconds\n",
      "Training Epoch: 9 - Time: 6.14 seconds\n",
      "Training Epoch: 10 - Time: 6.14 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Using', device)\n",
    "model = CassieNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Training Epoch: {} - Time: {:.2f} seconds'.format(epoch, time.time() - start_time))\n",
    "\n",
    "for epoch in range(1, 11): # Train for 10 epochs for demonstration\n",
    "    train(model, device, train_loader, optimizer, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0002, Accuracy: 9526/10000 (95%)\n"
     ]
    }
   ],
   "source": [
    "test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: there are multiple ways to approach this, and there is no one correct solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
