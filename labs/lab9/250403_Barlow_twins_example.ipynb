{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c17995-747f-4c81-9bda-8c47ed471e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/MaxLikesMath/Barlow-Twins-Pytorch/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9631757e-5738-4c91-9dc3-35038ae998e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImplementation of Barlow Twins (https://arxiv.org/abs/2103.03230), adapted for ease of use for experiments from\\nhttps://github.com/facebookresearch/barlowtwins, with some modifications using code from \\nhttps://github.com/lucidrains/byol-pytorch\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "'''\n",
    "Implementation of Barlow Twins (https://arxiv.org/abs/2103.03230), adapted for ease of use for experiments from\n",
    "https://github.com/facebookresearch/barlowtwins, with some modifications using code from \n",
    "https://github.com/lucidrains/byol-pytorch\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e13ff50-a86d-4160-b83e-86c04d9d492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def flatten(t):\n",
    "    return t.reshape(t.shape[0], -1)\n",
    "\n",
    "class NetWrapper(nn.Module):\n",
    "    # from https://github.com/lucidrains/byol-pytorch\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = flatten(output)\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def get_representation(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        representation = self.get_representation(x)\n",
    "\n",
    "        return representation\n",
    "\n",
    "\n",
    "\n",
    "def off_diagonal(x):\n",
    "    # return a flattened view of the off-diagonal elements of a square matrix\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "\n",
    "class BarlowTwins(nn.Module):\n",
    "    '''\n",
    "    Adapted from https://github.com/facebookresearch/barlowtwins for arbitrary backbones, and arbitrary choice of which\n",
    "    latent representation to use. Designed for models which can fit on a single GPU (though training can be parallelized\n",
    "    across multiple as with any other model). Support for larger models can be done easily for individual use cases by\n",
    "    by following PyTorch's model parallelism best practices.\n",
    "    '''\n",
    "    def __init__(self, backbone, latent_id, projection_sizes, lambd, scale_factor=1):\n",
    "        '''\n",
    "\n",
    "        :param backbone: Model backbone\n",
    "        :param latent_id: name (or index) of the layer to be fed to the projection MLP\n",
    "        :param projection_sizes: size of the hidden layers in the projection MLP\n",
    "        :param lambd: tradeoff function\n",
    "        :param scale_factor: Factor to scale loss by, default is 1\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.backbone = NetWrapper(self.backbone, latent_id)\n",
    "        self.lambd = lambd\n",
    "        self.scale_factor = scale_factor\n",
    "        # projector\n",
    "        sizes = projection_sizes\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=False))\n",
    "            layers.append(nn.BatchNorm1d(sizes[i + 1]))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Linear(sizes[-2], sizes[-1], bias=False))\n",
    "        self.projector = nn.Sequential(*layers)\n",
    "\n",
    "        # normalization layer for the representations z1 and z2\n",
    "        self.bn = nn.BatchNorm1d(sizes[-1], affine=False)\n",
    "\n",
    "    def forward(self, y1, y2):\n",
    "        z1 = self.backbone(y1)\n",
    "        z2 = self.backbone(y2)\n",
    "        z1 = self.projector(z1)\n",
    "        z2 = self.projector(z2)\n",
    "\n",
    "        # empirical cross-correlation matrix\n",
    "        c = torch.mm(self.bn(z1).T, self.bn(z2))\n",
    "        c.div_(z1.shape[0])\n",
    "\n",
    "\n",
    "        # use --scale-loss to multiply the loss by a constant factor\n",
    "        # see the Issues section of the readme\n",
    "        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n",
    "        off_diag = off_diagonal(c).pow_(2).sum()\n",
    "        loss = self.scale_factor*(on_diag + self.lambd * off_diag)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d225c8-6cd1-407e-b78f-d99374c0b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(128094.7344, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torchvision\n",
    "\n",
    "\n",
    "model = torchvision.models.resnet18(zero_init_residual=True)\n",
    "proj = [512, 512, 512, 512]\n",
    "twins = BarlowTwins(model, 'avgpool', proj, 0.5)\n",
    "inp1 = torch.rand(2,3,224,224)\n",
    "inp2 = torch.rand(2,3,224,224)\n",
    "outs =twins(inp1, inp2)\n",
    "#model = model_utils.extract_latent.LatentHook(model, ['avgpool'])\n",
    "#out, dicti = model(inp1)\n",
    "print(outs)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fa3e54-8a1e-47b0-9133-bd4fc6fd167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4785e35-5a09-41e1-9389-a5c34a6ea3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c69041e2-390f-48fa-8515-bfb617d48401",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=256\n",
    "batch_size = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06409a1a-cd32-4f51-aaff-6702a2056026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import torchvision.transforms as transforms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc6568e7-f218-4203-8d92-96280076dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=224*224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48bc60e0-0c9d-454e-b5cd-ca2fb019c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "'''\n",
    "#####\n",
    "Adapted from https://github.com/facebookresearch/barlowtwins\n",
    "#####\n",
    "'''\n",
    "\n",
    "\n",
    "class GaussianBlur(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            sigma = random.random() * 1.9 + 0.1\n",
    "            return img.filter(ImageFilter.GaussianBlur(sigma))\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "\n",
    "class Solarization(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            return ImageOps.solarize(img)\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "\n",
    "class Transform:\n",
    "    def __init__(self, transform=None, transform_prime=None):\n",
    "        '''\n",
    "\n",
    "        :param transform: Transforms to be applied to first input\n",
    "        :param transform_prime: transforms to be applied to second\n",
    "        '''\n",
    "        if transform == None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224, interpolation=Image.BICUBIC),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomApply(\n",
    "                    [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                            saturation=0.2, hue=0.1)],\n",
    "                    p=0.8\n",
    "                ),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                GaussianBlur(p=1.0),\n",
    "                Solarization(p=0.0),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "        if transform_prime == None:\n",
    "\n",
    "            self.transform_prime = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224, interpolation=Image.BICUBIC),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomApply(\n",
    "                    [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                            saturation=0.2, hue=0.1)],\n",
    "                    p=0.8\n",
    "                ),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                GaussianBlur(p=0.1),\n",
    "                Solarization(p=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform_prime = transform_prime\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y1 = self.transform(x)\n",
    "        y2 = self.transform_prime(x)\n",
    "        return y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7285661f-059e-45c2-9709-44b3a86f79f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5d983d2-1421-4f30-9c11-b2e21350a59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "image_size=[28,28]\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize((224,224)),\n",
    "                transforms.RandomResizedCrop(image_size,\n",
    "                                            interpolation=Image.BICUBIC),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomApply(\n",
    "                    [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                            saturation=0.2, hue=0.1)],\n",
    "                    p=0.8\n",
    "                ),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                GaussianBlur(p=1.0),\n",
    "                Solarization(p=0.0),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "# --- data loading --- #\n",
    "train_data = datasets.CIFAR10('./data', train=True, download=True,\n",
    "                            transform=transform)\n",
    "test_data = datasets.CIFAR10('./data', train=False,\n",
    "                           transform=transform)\n",
    "# pin memory provides improved transfer speed\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=BATCH_SIZE, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "266e4757-406d-4ef0-abd0-b64892ead9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.Resize((224,224)),\n",
    "                transforms.RandomResizedCrop(image_size,\n",
    "                                            interpolation=Image.BICUBIC),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomApply(\n",
    "                    [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                            saturation=0.2, hue=0.1)],\n",
    "                    p=0.8\n",
    "                ),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                GaussianBlur(p=1.0),\n",
    "                Solarization(p=0.0),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "\n",
    "#For the transform argument for the dataset, pass in \n",
    "# Twins.transform_utils.Transform(transform_1, transform_2)\n",
    "#If transforms are None, the Imagenet default is used.\n",
    "dataset = datasets.CIFAR10('./data', train=True, download=True,transform=Transform(transform, transform))\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b60d938d-b59c-4eab-b5e0-5c2d38c45caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3, 28, 28])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7d038b8-9c1e-416d-8a13-81371416ce86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3, 28, 28])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27b46cce-5c1d-4619-95f7-1596a5537e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "#This is just any generic model\n",
    "model = torchvision.models.resnet18(zero_init_residual=True)\n",
    "\n",
    "#Optional: define transformations for your specific dataset.\n",
    "#Generally, it is best to use the original augmentations in the\n",
    "#paper, replacing the Imagenet normalization with the normalization\n",
    "#for your dataset.\n",
    "\n",
    "#Make the BT instance, passing the model, the latent rep layer id,\n",
    "# hidden units for the projection MLP, the tradeoff factor,\n",
    "# and the loss scale.\n",
    "model = torchvision.models.resnet18(zero_init_residual=True)\n",
    "\n",
    "learner = BarlowTwins(model, 'avgpool', [512,1024, 1024, 1024],\n",
    "                      3.9e-3, 1)\n",
    "\n",
    "optimizer = torch.optim.Adam(learner.parameters(), lr=0.001)\n",
    "\n",
    "#Single training epoch\n",
    "for batch_idx, ((x1,x2), _) in enumerate(loader):\n",
    "    print(batch_idx)\n",
    "    loss = learner(x1, x2)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9176731-d6ea-47b6-a86e-8de8a7ebd245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
