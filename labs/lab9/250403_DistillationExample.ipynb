{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd97d13-7133-4357-bd88-a7761fc352b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/phlippe/uvadlc_notebooks/tree/master\n",
    "#https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial17/SimCLR.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d959d1e5-02bf-40ed-8284-615bd9d5b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Check if the current `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__\n",
    "# is available, and if not, use the CPU\n",
    "device = 'cuda'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f790691b-6a34-42ca-a4ed-23b624b196ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
    "transforms_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Loading the CIFAR-10 dataset:\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367bbe5c-3635-4ec7-adfa-f6bccf0cbc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66bc431b-6665-46cf-afb1-17f0def3bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper neural network class to be used as teacher:\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Lightweight neural network class to be used as student:\n",
    "class LightNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LightNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d8e98d-316a-4413-b0b2-83a23998ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, learning_rate, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # inputs: A collection of batch_size images\n",
    "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
    "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb63510-7054-4f88-a312-1810c2d6c8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3289154024075365\n",
      "Epoch 2/10, Loss: 0.8643754005737012\n",
      "Epoch 3/10, Loss: 0.67783413701655\n",
      "Epoch 4/10, Loss: 0.5343894877702074\n",
      "Epoch 5/10, Loss: 0.41320801429126575\n",
      "Epoch 6/10, Loss: 0.30617602596350035\n",
      "Epoch 7/10, Loss: 0.2203222609618131\n",
      "Epoch 8/10, Loss: 0.16668520262822165\n",
      "Epoch 9/10, Loss: 0.14027776430977884\n",
      "Epoch 10/10, Loss: 0.12077616061300725\n",
      "Test Accuracy: 74.97%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "nn_deep = DeepNN(num_classes=10).to(device)\n",
    "train(nn_deep, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
    "\n",
    "# Instantiate the lightweight network:\n",
    "torch.manual_seed(42)\n",
    "nn_light = LightNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02004029-b9e6-4ced-8021-db56663183ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "new_nn_light = LightNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad14a14e-90cf-46f4-8129-5b4a06abf90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of 1st layer of nn_light: 2.327361822128296\n",
      "Norm of 1st layer of new_nn_light: 2.327361822128296\n"
     ]
    }
   ],
   "source": [
    "# Print the norm of the first layer of the initial lightweight model\n",
    "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
    "# Print the norm of the first layer of the new lightweight model\n",
    "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e839ed65-741d-4a09-950a-78d0593d5513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNN parameters: 1,186,986\n",
      "LightNN parameters: 267,738\n"
     ]
    }
   ],
   "source": [
    "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
    "print(f\"DeepNN parameters: {total_params_deep}\")\n",
    "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
    "print(f\"LightNN parameters: {total_params_light}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ae7ca7d-87ce-441d-99fe-406af706c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.4701012848588206\n",
      "Epoch 2/10, Loss: 1.1596995148512408\n",
      "Epoch 3/10, Loss: 1.0293147256002402\n",
      "Epoch 4/10, Loss: 0.9266254234192012\n",
      "Epoch 5/10, Loss: 0.8503463513710919\n",
      "Epoch 6/10, Loss: 0.7839138437719906\n",
      "Epoch 7/10, Loss: 0.7166611834255325\n",
      "Epoch 8/10, Loss: 0.6579107024023295\n",
      "Epoch 9/10, Loss: 0.6030574823584398\n",
      "Epoch 10/10, Loss: 0.5513229549998213\n",
      "Test Accuracy: 70.17%\n"
     ]
    }
   ],
   "source": [
    "train(nn_light, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_light_ce = test(nn_light, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a5ecd84-1636-49f2-922c-ebdcd8143b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher accuracy: 74.97%\n",
      "Student accuracy: 70.17%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11839e88-d269-47da-9331-8830bad7dc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.39727075569465\n",
      "Epoch 2/10, Loss: 1.8822257817553742\n",
      "Epoch 3/10, Loss: 1.6551105970007074\n",
      "Epoch 4/10, Loss: 1.4959588395360182\n",
      "Epoch 5/10, Loss: 1.3653922765456197\n",
      "Epoch 6/10, Loss: 1.2556690922783464\n",
      "Epoch 7/10, Loss: 1.160513656675968\n",
      "Epoch 8/10, Loss: 1.0732086697197936\n",
      "Epoch 9/10, Loss: 1.003779393480257\n",
      "Epoch 10/10, Loss: 0.9256256629743844\n",
      "Test Accuracy: 70.94%\n",
      "Teacher accuracy: 74.97%\n",
      "Student accuracy without teacher: 70.17%\n",
      "Student accuracy with CE + KD: 70.94%\n"
     ]
    }
   ],
   "source": [
    "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            #Soften the student logits by applying softmax first and log() second\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
    "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n",
    "\n",
    "# Compare the student test accuracy with and without the teacher, after distillation\n",
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "066e33fd-a5ac-45a7-a9a8-2f8b2eaeca8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of 1st layer for deep_nn: 7.492077350616455\n",
      "Norm of 1st layer for modified_deep_nn: 7.492077350616455\n",
      "Norm of 1st layer: 2.327361822128296\n"
     ]
    }
   ],
   "source": [
    "class ModifiedDeepNNCosine(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedDeepNNCosine, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        flattened_conv_output = torch.flatten(x, 1)\n",
    "        x = self.classifier(flattened_conv_output)\n",
    "        flattened_conv_output_after_pooling = torch.nn.functional.avg_pool1d(flattened_conv_output, 2)\n",
    "        return x, flattened_conv_output_after_pooling\n",
    "\n",
    "# Create a similar student class where we return a tuple. We do not apply pooling after flattening.\n",
    "class ModifiedLightNNCosine(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedLightNNCosine, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        flattened_conv_output = torch.flatten(x, 1)\n",
    "        x = self.classifier(flattened_conv_output)\n",
    "        return x, flattened_conv_output\n",
    "\n",
    "# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
    "modified_nn_deep = ModifiedDeepNNCosine(num_classes=10).to(device)\n",
    "modified_nn_deep.load_state_dict(nn_deep.state_dict())\n",
    "\n",
    "# Once again ensure the norm of the first layer is the same for both networks\n",
    "print(\"Norm of 1st layer for deep_nn:\", torch.norm(nn_deep.features[0].weight).item())\n",
    "print(\"Norm of 1st layer for modified_deep_nn:\", torch.norm(modified_nn_deep.features[0].weight).item())\n",
    "\n",
    "# Initialize a modified lightweight network with the same seed as our other lightweight instances. This will be trained from scratch to examine the effectiveness of cosine loss minimization.\n",
    "torch.manual_seed(42)\n",
    "modified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)\n",
    "print(\"Norm of 1st layer:\", torch.norm(modified_nn_light.features[0].weight).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e9255de-ea0e-4364-be84-433d9468e820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student logits shape: torch.Size([128, 10])\n",
      "Student hidden representation shape: torch.Size([128, 1024])\n",
      "Teacher logits shape: torch.Size([128, 10])\n",
      "Teacher hidden representation shape: torch.Size([128, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Create a sample input tensor\n",
    "sample_input = torch.randn(128, 3, 32, 32).to(device) # Batch size: 128, Filters: 3, Image size: 32x32\n",
    "\n",
    "# Pass the input through the student\n",
    "logits, hidden_representation = modified_nn_light(sample_input)\n",
    "\n",
    "# Print the shapes of the tensors\n",
    "print(\"Student logits shape:\", logits.shape) # batch_size x total_classes\n",
    "print(\"Student hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size\n",
    "\n",
    "# Pass the input through the teacher\n",
    "logits, hidden_representation = modified_nn_deep(sample_input)\n",
    "\n",
    "# Print the shapes of the tensors\n",
    "print(\"Teacher logits shape:\", logits.shape) # batch_size x total_classes\n",
    "print(\"Teacher hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e9f2c48-69f1-4637-9449-dace192b8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cosine_loss(teacher, student, train_loader, epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    cosine_loss = nn.CosineEmbeddingLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.to(device)\n",
    "    student.to(device)\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model and keep only the hidden representation\n",
    "            with torch.no_grad():\n",
    "                _, teacher_hidden_representation = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits, student_hidden_representation = student(inputs)\n",
    "\n",
    "            # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.\n",
    "            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=torch.ones(inputs.size(0)).to(device))\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2a8868c-20da-48e5-bb54-8f5462f5a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multiple_outputs(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs, _ = model(inputs) # Disregard the second tensor of the tuple\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62a72248-2172-473b-90c3-e903c6d48ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3080871385686539\n",
      "Epoch 2/10, Loss: 1.0733524277387068\n",
      "Epoch 3/10, Loss: 0.9756805963833314\n",
      "Epoch 4/10, Loss: 0.9003840056831575\n",
      "Epoch 5/10, Loss: 0.8460081969685567\n",
      "Epoch 6/10, Loss: 0.8019529346309965\n",
      "Epoch 7/10, Loss: 0.7582531089672957\n",
      "Epoch 8/10, Loss: 0.7206244671436222\n",
      "Epoch 9/10, Loss: 0.6837849459989601\n",
      "Epoch 10/10, Loss: 0.6602115329269254\n",
      "Test Accuracy: 70.77%\n"
     ]
    }
   ],
   "source": [
    "# Train and test the lightweight network with cross entropy loss\n",
    "train_cosine_loss(teacher=modified_nn_deep, student=modified_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, hidden_rep_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_light_ce_and_cosine_loss = test_multiple_outputs(modified_nn_light, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "388cae8d-0122-4435-9e20-038f545ba78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student's feature extractor output shape:  torch.Size([128, 16, 8, 8])\n",
      "Teacher's feature extractor output shape:  torch.Size([128, 32, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# Pass the sample input only from the convolutional feature extractor\n",
    "convolutional_fe_output_student = nn_light.features(sample_input)\n",
    "convolutional_fe_output_teacher = nn_deep.features(sample_input)\n",
    "\n",
    "# Print their shapes\n",
    "print(\"Student's feature extractor output shape: \", convolutional_fe_output_student.shape)\n",
    "print(\"Teacher's feature extractor output shape: \", convolutional_fe_output_teacher.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dd30ba1-8123-4966-b9f8-5bc46d2e303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedDeepNNRegressor(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedDeepNNRegressor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        conv_feature_map = x\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x, conv_feature_map\n",
    "\n",
    "class ModifiedLightNNRegressor(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedLightNNRegressor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # Include an extra regressor (in our case linear)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        regressor_output = self.regressor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x, regressor_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68d97356-b416-4fb6-bd69-665eb30f85b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.6956973225259415\n",
      "Epoch 2/10, Loss: 1.3220868970427062\n",
      "Epoch 3/10, Loss: 1.180282173864067\n",
      "Epoch 4/10, Loss: 1.0875657005688113\n",
      "Epoch 5/10, Loss: 1.01077735744169\n",
      "Epoch 6/10, Loss: 0.9500702744554681\n",
      "Epoch 7/10, Loss: 0.8963907981467674\n",
      "Epoch 8/10, Loss: 0.8487572697422389\n",
      "Epoch 9/10, Loss: 0.8072099949392821\n",
      "Epoch 10/10, Loss: 0.7689305871648862\n",
      "Test Accuracy: 70.95%\n"
     ]
    }
   ],
   "source": [
    "def train_mse_loss(teacher, student, train_loader, epochs, learning_rate, feature_map_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.to(device)\n",
    "    student.to(device)\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Again ignore teacher logits\n",
    "            with torch.no_grad():\n",
    "                _, teacher_feature_map = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits, regressor_feature_map = student(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            hidden_rep_loss = mse_loss(regressor_feature_map, teacher_feature_map)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = feature_map_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Notice how our test function remains the same here with the one we used in our previous case. We only care about the actual outputs because we measure accuracy.\n",
    "\n",
    "# Initialize a ModifiedLightNNRegressor\n",
    "torch.manual_seed(42)\n",
    "modified_nn_light_reg = ModifiedLightNNRegressor(num_classes=10).to(device)\n",
    "\n",
    "# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
    "modified_nn_deep_reg = ModifiedDeepNNRegressor(num_classes=10).to(device)\n",
    "modified_nn_deep_reg.load_state_dict(nn_deep.state_dict())\n",
    "\n",
    "# Train and test once again\n",
    "train_mse_loss(teacher=modified_nn_deep_reg, student=modified_nn_light_reg, train_loader=train_loader, epochs=10, learning_rate=0.001, feature_map_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_light_ce_and_mse_loss = test_multiple_outputs(modified_nn_light_reg, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e14bf33e-7635-4232-b09b-12e95f98d2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher accuracy: 74.97%\n",
      "Student accuracy without teacher: 70.17%\n",
      "Student accuracy with CE + KD: 70.94%\n",
      "Student accuracy with CE + CosineLoss: 70.77%\n",
      "Student accuracy with CE + RegressorMSE: 70.95%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\n",
    "print(f\"Student accuracy with CE + CosineLoss: {test_accuracy_light_ce_and_cosine_loss:.2f}%\")\n",
    "print(f\"Student accuracy with CE + RegressorMSE: {test_accuracy_light_ce_and_mse_loss:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e356d-58ee-4163-92d5-6c1fc39183fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/data-science/distillation-of-bert-like-models-the-code-73c31e8c2b0a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3e4e675-72f1-4d5d-bbfb-5fa969866b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424374c8e176491ebfd6367acd1f30fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 07:58:06.279102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743681486.303702 2874558 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743681486.312460 2874558 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-03 07:58:06.344222: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2605f8b39b3845dfac398e4c1158d027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "roberta = AutoModelForMaskedLM.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28e1698d-d6ba-4cef-8f39-40a2d005a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel, RobertaConfig\n",
    "\n",
    "def distill_roberta(\n",
    "    teacher_model : RobertaPreTrainedModel,\n",
    ") -> RobertaPreTrainedModel:\n",
    "    \"\"\"\n",
    "    Distilates a RoBERTa (teacher_model) like would DistilBERT for a BERT model.\n",
    "    The student model has the same configuration, except for the number of hidden layers, which is // by 2.\n",
    "    The student layers are initilized by copying one out of two layers of the teacher, starting with layer 0.\n",
    "    The head of the teacher is also copied.\n",
    "    \"\"\"\n",
    "    # Get teacher configuration as a dictionnary\n",
    "    configuration = teacher_model.config.to_dict()\n",
    "    # Half the number of hidden layer\n",
    "    configuration['num_hidden_layers'] //= 2\n",
    "    # Convert the dictionnary to the student configuration\n",
    "    configuration = RobertaConfig.from_dict(configuration)\n",
    "    # Create uninitialized student model\n",
    "    student_model = type(teacher_model)(configuration)\n",
    "    # Initialize the student's weights\n",
    "    distill_roberta_weights(teacher=teacher_model, student=student_model)\n",
    "    # Return the student model\n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "559b797b-97d5-46ba-9a63-fc029cc66de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaEncoder, RobertaModel\n",
    "from torch.nn import Module\n",
    "\n",
    "def distill_roberta_weights(\n",
    "    teacher : Module,\n",
    "    student : Module,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Recursively copies the weights of the (teacher) to the (student).\n",
    "    This function is meant to be first called on a RobertaFor... model, but is then called on every children of that model recursively.\n",
    "    The only part that's not fully copied is the encoder, of which only half is copied.\n",
    "    \"\"\"\n",
    "    # If the part is an entire RoBERTa model or a RobertaFor..., unpack and iterate\n",
    "    if isinstance(teacher, RobertaModel) or type(teacher).__name__.startswith('RobertaFor'):\n",
    "        for teacher_part, student_part in zip(teacher.children(), student.children()):\n",
    "            distill_roberta_weights(teacher_part, student_part)\n",
    "    # Else if the part is an encoder, copy one out of every layer\n",
    "    elif isinstance(teacher, RobertaEncoder):\n",
    "            teacher_encoding_layers = [layer for layer in next(teacher.children())]\n",
    "            student_encoding_layers = [layer for layer in next(student.children())]\n",
    "            for i in range(len(student_encoding_layers)):\n",
    "                student_encoding_layers[i].load_state_dict(teacher_encoding_layers[2*i].state_dict())\n",
    "    # Else the part is a head or something else, copy the state_dict\n",
    "    else:\n",
    "        student.load_state_dict(teacher.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d800c5e-7467-40d9-9d79-edd7b4e34d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "def get_logits(\n",
    "    model : RobertaPreTrainedModel, \n",
    "    input_ids : Tensor,\n",
    "    attention_mask : Tensor,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Given a RoBERTa (model) for classification and the couple of (input_ids) and (attention_mask),\n",
    "    returns the logits corresponding to the prediction.\n",
    "    \"\"\"\n",
    "    return model.classifier(\n",
    "        model.roberta(input_ids, attention_mask)[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5f50176-ff6c-43bd-ba97-72ef7408e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, CosineEmbeddingLoss\n",
    "\n",
    "def distillation_loss(\n",
    "    teacher_logits : Tensor,\n",
    "    student_logits : Tensor,\n",
    "    labels : Tensor,\n",
    "    temperature : float = 1.0,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    The distillation loss for distilating a BERT-like model.\n",
    "    The loss takes the (teacher_logits), (student_logits) and (labels) for various losses.\n",
    "    The (temperature) can be given, otherwise it's set to 1 by default.\n",
    "    \"\"\"\n",
    "    # Temperature and sotfmax\n",
    "    student_logits, teacher_logits = (student_logits / temperature).softmax(1), (teacher_logits / temperature).softmax(1)\n",
    "    # Classification loss (problem-specific loss)\n",
    "    loss = CrossEntropyLoss()(student_logits, labels)\n",
    "    # CrossEntropy teacher-student loss\n",
    "    loss = loss + CrossEntropyLoss()(student_logits, teacher_logits)\n",
    "    # Cosine loss\n",
    "    loss = loss + CosineEmbeddingLoss()(teacher_logits, student_logits, torch.ones(teacher_logits.size()[0]))\n",
    "    # Average the loss and return it\n",
    "    loss = loss / 3\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d60f4d-7d5c-4a9b-a04d-4328e85c8529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ab59847870433ea071ff3561ce9c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51de340b7304ce3a4999e114aff0452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openwebtext.py:   0%|          | 0.00/2.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/openwebtext.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ae6feed2824c25a653cfa317da2b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/21 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3c7eee15d2421ba9251df411204dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset00.tar:   0%|          | 0.00/633M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5ff532fa504eeab44a58582ed479e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset01.tar:   0%|          | 0.00/629M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b37798921fe4999b87ada8ac1b99d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset02.tar:   0%|          | 0.00/629M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667973e674b94e84b34ccaeaf12f57df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset03.tar:   0%|          | 0.00/628M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa9fe2224594ad4bfbf366c5406bbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset04.tar:   0%|          | 0.00/627M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e7a3af0357404d8b29c9e7a0099a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset05.tar:   0%|          | 0.00/630M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ec25a2673f48d1a575c9f6bf7d22c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset06.tar:   0%|          | 0.00/626M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3883e3e1a4415a9e9518c8ab2921cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset07.tar:   0%|          | 0.00/625M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acff3265d2ce40c2b3607bb43f1c5021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset08.tar:   0%|          | 0.00/625M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66e32d69fff46abba68bb960ca05114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset09.tar:   0%|          | 0.00/626M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b6462f45d24af7ad403871d61b823d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset10.tar:   0%|          | 0.00/625M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5582737b46a246f88aecca8fafc73e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset11.tar:   0%|          | 0.00/625M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4929242f8284482498283cfd95fb98d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset12.tar:   0%|          | 0.00/624M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27c795bb152465298a0f1efdf2465ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset13.tar:   0%|          | 0.00/629M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c61006229f42a788f787658013003d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset14.tar:   0%|          | 0.00/627M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee2d57336944677a832d0d1b1f919c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset15.tar:   0%|          | 0.00/621M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ac607dd770469e8add655676f0ecd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset16.tar:   0%|          | 0.00/619M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae02b5a7a3d64697a904495b5f1e1bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset17.tar:   0%|          | 0.00/619M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871553d7e7cd4145bb75ab0db37b615c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset18.tar:   0%|          | 0.00/618M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf703fbbf53046bf93d81208c7a7f79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset19.tar:   0%|          | 0.00/619M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37eda23ae3dc4cb4abe271d868667813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "urlsf_subset20.tar:   0%|          | 0.00/377M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519dcc0fcdcd4f81afdbe57ae7ac89a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8013769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"openwebtext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9146f4-c03c-41a0-971c-d38ab77a8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "paths = [str(x) for x in Path('data/original').glob('**/*.txt')]\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "        clean_text=True,\n",
    "        handle_chinese_chars=False,\n",
    "        strip_accents=False,\n",
    "        lowercase=True\n",
    ")\n",
    "tokenizer.train(files=paths[:10], vocab_size=30_000, min_frequency=2,\n",
    "                    limit_alphabet=1000, wordpieces_prefix='##',\n",
    "                    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7120d-3b33-449a-912b-8e77817b393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer('Hey, how are you?')\n",
    "print(tokens)\n",
    "tokenizer.decode(tokens['input_ids']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9c517-a592-48d3-a706-61ae213077a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer('Hey, how are you broski?')\n",
    "print(tokens)\n",
    "tokenizer.decode(tokens['input_ids']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e80337-4d95-4b25-b85c-c6bcd3a26e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(paths = [str(x) for x in Path('data/original').glob('**/*.txt')][50:70], tokenizer=tokenizer)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
    "\n",
    "test_dataset = Dataset(paths = [str(x) for x in Path('data/original').glob('**/*.txt')][10:12], tokenizer=tokenizer)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6dbbe-b605-4bbd-bee9-793297c28028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForMaskedLM, DistilBertConfig\n",
    "\n",
    "config = DistilBertConfig(\n",
    "    vocab_size=30000,\n",
    "    max_position_embeddings=514\n",
    ")\n",
    "model = DistilBertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375aa2d-7005-44c7-8f77-8c8745194581",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    \n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    # iterate over dataset\n",
    "    for batch in loop:\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # copy input to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # predict\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        # update weights\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        \n",
    "        # output current loss\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(\"Mean Training Loss\", np.mean(losses))\n",
    "    losses = []\n",
    "    loop = tqdm(test_loader, leave=True)\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # iterate over dataset\n",
    "    for batch in loop:\n",
    "        # copy input to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # predict\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        # update weights\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # output current loss\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "    print(\"Mean Test Loss\", np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d951fbb2-79ef-4f13-bb75-bad0f3d4bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill = pipeline(\"fill-mask\", model='distilbert', config=config, tokenizer='distilbert_tokenizer')\n",
    "fill(f'It seems important to tackle the climate {fill.tokenizer.mask_token}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
