{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#http://clam.mahmoodlab.org/\n",
        "\n",
        "#SSL models for hOptimus"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1d3a2b16-17bf-44cc-9485-e1dde77f70de"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os"
      ],
      "execution_count": 1,
      "outputs": [],
      "id": "7b4feaee-e2f0-4884-b35d-4b5bf60ab093"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hf_token = os.environ['HUGGINGFACE_TOKEN']"
      ],
      "execution_count": 3,
      "outputs": [],
      "id": "d4797795-86c8-4b7a-bd6a-bde2b029ac6d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from huggingface_hub import login\n",
        "import torch\n",
        "import timm \n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# Login to the Hugging Face hub, using your user access token that can be found here:\n",
        "# https://huggingface.co/settings/tokens.\n",
        "login(hf_token)\n",
        "\n",
        "model = timm.create_model(\n",
        "    \"hf-hub:bioptimus/H-optimus-0\", pretrained=True, init_values=1e-5, dynamic_img_size=False\n",
        ")\n",
        "model.to(\"cuda\")\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=(0.707223, 0.578729, 0.703617), \n",
        "        std=(0.211883, 0.230117, 0.177517)\n",
        "    ),\n",
        "])\n",
        "\n",
        "input = torch.rand(3, 224, 224)\n",
        "input = transforms.ToPILImage()(input)\n",
        "\n",
        "# We recommend using mixed precision for faster inference.\n",
        "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "    with torch.inference_mode():\n",
        "        features = model(transform(input).unsqueeze(0).to(\"cuda\"))\n",
        "\n",
        "assert features.shape == (1, 1536)\n"
      ],
      "execution_count": 4,
      "outputs": [],
      "id": "b5c28de0-74dc-4b01-ae08-00466003527f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "def download_tcga_pathology():\n",
        "    \"\"\"\n",
        "    Downloads TCGA pathology reports for downstream analysis.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: TCGA pathology reports\n",
        "    \"\"\"\n",
        "    # Downloads TCGA pathology reports for downstream analysis.\n",
        "    os.system(\"curl -L https://data.mendeley.com/public-files/datasets/hyg5xkznpx/files/60abe141-9352-4a54-943c-3d015eabefea/file_downloaded --output TCGA_Reports.csv.zip\")\n",
        "    os.system(\"unzip -qq -o TCGA_Reports.csv.zip\")\n",
        "    tcga_reports = pd.read_csv(\"TCGA_Reports.csv\")\n",
        "    return tcga_reports\n",
        "# Downloads TCGA pathology reports and extracts simplified disease phrases.\n",
        "os.chdir(\"/gpfs/scratch/nk4167/TCGA_Path\")\n",
        "path_use=1\n",
        "tcga_reports = download_tcga_pathology()\n",
        "res_all = []\n",
        "for i in range(path_use):\n",
        "    patient_name = tcga_reports['patient_filename'][i].split('.')[0]\n",
        "    text = tcga_reports['text'][i]\n",
        "    break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   134  100   134    0     0    266      0 --:--:-- --:--:-- --:--:--   266\n",
            "100 10.4M  100 10.4M    0     0  5155k      0  0:00:02  0:00:02 --:--:-- 8693k\n"
          ]
        }
      ],
      "id": "3b7169a8-7f0a-4218-b11e-0dd5416b283b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Custom WSI Reader and Tiler Implementation (replacing tiatoolbox)\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "try:\n",
        "    import cv2\n",
        "    HAS_CV2 = True\n",
        "except ImportError:\n",
        "    HAS_CV2 = False\n",
        "    print(\"OpenCV not available, using basic thresholding for tissue mask\")\n",
        "from typing import Tuple, Iterator, Optional\n",
        "\n",
        "class CustomWSIReader:\n",
        "    \"\"\"Custom WSI reader that works with SVS files using openslide or PIL fallback\"\"\"\n",
        "    def __init__(self, svs_path: str):\n",
        "        self.svs_path = svs_path\n",
        "        try:\n",
        "            import openslide\n",
        "            self.slide = openslide.OpenSlide(svs_path)\n",
        "            self.use_openslide = True\n",
        "            # Get level 0 dimensions (highest resolution)\n",
        "            self.level0_dimensions = self.slide.level_dimensions[0]\n",
        "            # Get microns per pixel if available\n",
        "            try:\n",
        "                mpp_x = float(self.slide.properties.get('openslide.mpp-x', 0.25))\n",
        "                mpp_y = float(self.slide.properties.get('openslide.mpp-y', 0.25))\n",
        "                self.mpp = (mpp_x + mpp_y) / 2\n",
        "            except:\n",
        "                self.mpp = 0.25  # Default assumption\n",
        "        except ImportError:\n",
        "            # Fallback to PIL for regular images\n",
        "            print(\"openslide not available, using PIL fallback\")\n",
        "            self.slide = Image.open(svs_path)\n",
        "            self.use_openslide = False\n",
        "            self.level0_dimensions = self.slide.size\n",
        "            self.mpp = 0.25  # Default assumption\n",
        "        \n",
        "        self.info = type('obj', (object,), {'mpp': [self.mpp]})()\n",
        "    \n",
        "    def slide_dimensions(self, resolution: float = 0.51, units: str = 'mpp') -> Tuple[int, int]:\n",
        "        \"\"\"Get slide dimensions at specified resolution\"\"\"\n",
        "        if units == 'mpp':\n",
        "            # Calculate scale factor\n",
        "            scale = self.mpp / resolution\n",
        "            w = int(self.level0_dimensions[0] * scale)\n",
        "            h = int(self.level0_dimensions[1] * scale)\n",
        "            return (w, h)\n",
        "        return self.level0_dimensions\n",
        "    \n",
        "    def read_region(self, location: Tuple[int, int], level: int, size: Tuple[int, int]) -> Image.Image:\n",
        "        \"\"\"Read a region from the slide\"\"\"\n",
        "        if self.use_openslide:\n",
        "            return self.slide.read_region(location, level, size)\n",
        "        else:\n",
        "            # For PIL, crop the image\n",
        "            return self.slide.crop((location[0], location[1], \n",
        "                                   location[0] + size[0], location[1] + size[1]))\n",
        "    \n",
        "    def get_thumbnail(self, size: Tuple[int, int] = (512, 512)) -> Image.Image:\n",
        "        \"\"\"Get a thumbnail of the slide\"\"\"\n",
        "        if self.use_openslide:\n",
        "            return self.slide.get_thumbnail(size)\n",
        "        else:\n",
        "            return self.slide.resize(size, Image.Resampling.LANCZOS)\n",
        "    \n",
        "    def tissue_mask(self, resolution: float = 1.0, units: str = \"power\") -> np.ndarray:\n",
        "        \"\"\"Create a simple tissue mask using color thresholding\"\"\"\n",
        "        # Get thumbnail for mask generation\n",
        "        thumb_size = (1024, 1024)\n",
        "        thumb = self.get_thumbnail(thumb_size)\n",
        "        thumb_np = np.array(thumb)\n",
        "        \n",
        "        # Convert to HSV for better color-based segmentation\n",
        "        if HAS_CV2 and len(thumb_np.shape) == 3:\n",
        "            hsv = cv2.cvtColor(thumb_np, cv2.COLOR_RGB2HSV)\n",
        "            # Create mask: exclude white/light background\n",
        "            # Tissue typically has lower brightness\n",
        "            lower_bound = np.array([0, 0, 0])\n",
        "            upper_bound = np.array([180, 255, 240])\n",
        "            mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
        "            # Resize mask to match slide dimensions if needed\n",
        "            target_dims = self.slide_dimensions(resolution=self.mpp, units='mpp')\n",
        "            mask_resized = cv2.resize(mask, target_dims, interpolation=cv2.INTER_NEAREST)\n",
        "        else:\n",
        "            # Fallback: simple brightness-based thresholding\n",
        "            if len(thumb_np.shape) == 3:\n",
        "                gray = np.mean(thumb_np, axis=2)\n",
        "            else:\n",
        "                gray = thumb_np\n",
        "            # Simple threshold: tissue is darker than background\n",
        "            threshold = np.percentile(gray, 80)  # Top 20% brightest = background\n",
        "            mask = (gray < threshold).astype(np.uint8) * 255\n",
        "            # Resize to target dimensions\n",
        "            target_dims = self.slide_dimensions(resolution=self.mpp, units='mpp')\n",
        "            mask_pil = Image.fromarray(mask).resize(target_dims, Image.Resampling.NEAREST)\n",
        "            mask_resized = np.array(mask_pil)\n",
        "        \n",
        "        return mask_resized / 255.0  # Normalize to 0-1\n",
        "\n",
        "\n",
        "class SlidingWindowPatchExtractor:\n",
        "    \"\"\"Custom sliding window patch extractor\"\"\"\n",
        "    def __init__(self, input_img: CustomWSIReader, patch_size: Tuple[int, int], \n",
        "                 stride: Tuple[int, int], input_mask: Optional[np.ndarray] = None,\n",
        "                 min_mask_ratio: float = 0.75):\n",
        "        self.input_img = input_img\n",
        "        self.patch_size = patch_size\n",
        "        self.stride = stride\n",
        "        self.input_mask = input_mask\n",
        "        self.min_mask_ratio = min_mask_ratio\n",
        "        \n",
        "        # Get image dimensions at target resolution\n",
        "        self.dims = input_img.slide_dimensions(resolution=input_img.mpp, units='mpp')\n",
        "        self.w, self.h = self.dims\n",
        "        \n",
        "        # Calculate number of patches\n",
        "        self.n_patches_x = (self.w - patch_size[0]) // stride[0] + 1\n",
        "        self.n_patches_y = (self.h - patch_size[1]) // stride[1] + 1\n",
        "        \n",
        "        self.patches = []\n",
        "        self._generate_patches()\n",
        "    \n",
        "    def _generate_patches(self):\n",
        "        \"\"\"Generate all valid patches\"\"\"\n",
        "        for y in range(0, self.h - self.patch_size[1] + 1, self.stride[1]):\n",
        "            for x in range(0, self.w - self.patch_size[0] + 1, self.stride[0]):\n",
        "                # Check mask if available\n",
        "                if self.input_mask is not None:\n",
        "                    patch_mask = self.input_mask[y:y+self.patch_size[1], x:x+self.patch_size[0]]\n",
        "                    mask_ratio = np.mean(patch_mask)\n",
        "                    if mask_ratio < self.min_mask_ratio:\n",
        "                        continue\n",
        "                \n",
        "                # Extract patch\n",
        "                patch = self.input_img.read_region((x, y), 0, self.patch_size)\n",
        "                # Convert to RGB if needed and resize to patch_size\n",
        "                if isinstance(patch, Image.Image):\n",
        "                    if patch.mode != 'RGB':\n",
        "                        patch = patch.convert('RGB')\n",
        "                    # Resize to ensure exact patch_size\n",
        "                    if patch.size != self.patch_size:\n",
        "                        patch = patch.resize(self.patch_size, Image.Resampling.LANCZOS)\n",
        "                \n",
        "                self.patches.append(patch)\n",
        "    \n",
        "    def __iter__(self):\n",
        "        \"\"\"Iterator interface\"\"\"\n",
        "        return iter(self.patches)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.patches)\n",
        "\n",
        "\n",
        "def download_bs_svs(patient_id, output_dir='./output'):\n",
        "    \"\"\"\n",
        "    Searches GDC for files matching BS slides and downloads them.\n",
        "    \n",
        "    Args:\n",
        "        patient_id (str): TCGA patient identifier\n",
        "        output_dir (str): Directory to save downloaded slides\n",
        "    \"\"\"\n",
        "    api_url = \"https://api.gdc.cancer.gov/files\"\n",
        "    query = {\n",
        "        \"filters\": {\n",
        "            \"op\": \"and\",\n",
        "            \"content\": [\n",
        "                {\"op\": \"=\", \"content\": {\"field\": \"cases.submitter_id\", \"value\": [patient_id]}},\n",
        "                {\"op\": \"=\", \"content\": {\"field\": \"data_format\", \"value\": \"SVS\"}},\n",
        "                {\"op\": \"=\", \"content\": {\"field\": \"file_name\", \"value\": \"*BS*\"}}  # Strict BS filter\n",
        "            ]\n",
        "        },\n",
        "        \"fields\": \"file_id,file_name\",\n",
        "        \"format\": \"JSON\",\n",
        "        \"size\": 1000\n",
        "    }\n",
        "\n",
        "    response = requests.post(api_url, json=query).json()\n",
        "    files = response.get('data', {}).get('hits', [])\n",
        "    \n",
        "    if not files:\n",
        "        raise ValueError(f\"No BS slides found for {patient_id}\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    downloaded_files = []\n",
        "    print(files)\n",
        "    for f in files:\n",
        "        # Builds the download URL and streams the file to disk.\n",
        "        dl_url = f\"https://api.gdc.cancer.gov/data/{f['file_id']}\"\n",
        "        path = os.path.join(output_dir, f['file_name'])\n",
        "        \n",
        "        with requests.get(dl_url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(path, 'wb') as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "        downloaded_files.append(path)\n",
        "    \n",
        "    return downloaded_files\n",
        "\n",
        "def generate_wsi_patches(wsi_image: CustomWSIReader, wsi_mask: np.ndarray, \n",
        "                         patch_size: int = 224, overlap: int = 256//8, \n",
        "                         output_dir: str = \"./output\", \n",
        "                         tissue_threshold: float = 0.75, \n",
        "                         resolution: float = 0.51, \n",
        "                         min_patches: int = 100):\n",
        "    \"\"\"\n",
        "    Generate overlapping tissue patches from WSIs with quality control.\n",
        "    \n",
        "    Args:\n",
        "        wsi_image: CustomWSIReader object for the image\n",
        "        wsi_mask: numpy array tissue mask\n",
        "        patch_size (int): Patch size in pixels (square)\n",
        "        overlap (int): Overlap between patches in pixels\n",
        "        output_dir (str): Directory to save patches\n",
        "        tissue_threshold (float): Minimum tissue content (0-1)\n",
        "        resolution (float): Microns per pixel (default 0.51 = 20x)\n",
        "        min_patches (int): Minimum number of patches to collect\n",
        "    \n",
        "    Returns:\n",
        "        SlidingWindowPatchExtractor: Patch generator object\n",
        "    \"\"\"\n",
        "    \n",
        "    # Verify matching dimensions at target resolution\n",
        "    img_dims = wsi_image.slide_dimensions(resolution=resolution, units='mpp')\n",
        "    print(f\"Image dimensions: {img_dims}\")\n",
        "    print(f\"Mask shape: {wsi_mask.shape}\")\n",
        "    \n",
        "    # Calculate stride and initialize patch generator\n",
        "    stride = patch_size - overlap\n",
        "    patch_gen = SlidingWindowPatchExtractor(\n",
        "        input_img=wsi_image,\n",
        "        patch_size=(patch_size, patch_size),\n",
        "        stride=(stride, stride),\n",
        "        input_mask=wsi_mask,\n",
        "        min_mask_ratio=tissue_threshold\n",
        "    )\n",
        "    return patch_gen\n",
        "    \n",
        "def download_slide(patient_id):\n",
        "    \"\"\"\n",
        "    Downloads a TCGA slide SVS and prepares patches for processing.\n",
        "    Output directory is named after the patient ID.\n",
        "    \n",
        "    Args:\n",
        "        patient_id (str): TCGA patient identifier (e.g., 'TCGA-BP-5195')\n",
        "\n",
        "    Returns:\n",
        "        tuple: Path to SVS, CustomWSIReader object, tissue mask, patch generator\n",
        "    \"\"\"\n",
        "    # Orchestrates download of slide SVS and preparation of patches.\n",
        "    svs_path = download_bs_svs(patient_id, output_dir=patient_id + '_output')[0]\n",
        "    wsi = CustomWSIReader(svs_path)\n",
        "    mask = wsi.tissue_mask(resolution=1, units=\"power\")\n",
        "    patch_gen = generate_wsi_patches(wsi, mask, resolution=wsi.mpp)\n",
        "    return (svs_path, wsi, mask, patch_gen)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8605cbd9-3cb7-4c62-a34d-522b0fce7fa8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "# Note: Custom WSI reader and tiler implementation replaces tiatoolbox\n",
        "# See the previous cell for CustomWSIReader and SlidingWindowPatchExtractor classes"
      ],
      "execution_count": 16,
      "outputs": [],
      "id": "288c1b29-52b7-4cba-94fc-c4e61d0a075b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        " svs_path,wsi,mask,patch_gen = download_slide(patient_name)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'id': 'ba4c46ba-08dc-4193-a8b2-20f20c1f7a76', 'file_name': 'TCGA-BP-5195-01A-02-BS2.f7182403-4158-4823-9eb7-70a11a6ae26c.svs', 'file_id': 'ba4c46ba-08dc-4193-a8b2-20f20c1f7a76'}, {'id': '333eab78-57c0-4e27-82fa-67dc068b91ec', 'file_name': 'TCGA-BP-5195-01A-01-BS1.6ea4093c-d6bb-4926-b2c3-b7b2d4c12689.svs', 'file_id': '333eab78-57c0-4e27-82fa-67dc068b91ec'}]\n",
            "[34001 31777]\n",
            "[34001 31777]\n"
          ]
        }
      ],
      "id": "417ad16d-9211-4b53-a8b7-e2cb750bdae1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from huggingface_hub import login\n",
        "import torch\n",
        "import timm \n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# Login to the Hugging Face hub, using your user access token that can be found here:\n",
        "# https://huggingface.co/settings/tokens.\n",
        "login(hf_token)\n",
        "\n",
        "model = timm.create_model(\n",
        "    \"hf-hub:bioptimus/H-optimus-0\", pretrained=True, init_values=1e-5, dynamic_img_size=False\n",
        ")\n",
        "model.to(\"cuda\")\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=(0.707223, 0.578729, 0.703617), \n",
        "        std=(0.211883, 0.230117, 0.177517)\n",
        "    ),\n",
        "])\n",
        "\n",
        "features_all = []\n",
        "# We recommend using mixed precision for faster inference.\n",
        "with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "    for patch in patch_gen:\n",
        "        # Ensure patch is a PIL Image\n",
        "        if isinstance(patch, np.ndarray):\n",
        "            patch = Image.fromarray(patch)\n",
        "        elif not isinstance(patch, Image.Image):\n",
        "            patch = Image.fromarray(np.array(patch))\n",
        "        \n",
        "        # Transform and process\n",
        "        input_tensor = transform(patch).unsqueeze(0).to(\"cuda\")\n",
        "        with torch.inference_mode():\n",
        "            features = model(input_tensor)\n",
        "            features_all.append(features.cpu())  # Move to CPU to save GPU memory"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|2025-04-03|05:51:09.812| [INFO] Loading pretrained weights from Hugging Face hub (bioptimus/H-optimus-0)\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 704.00 KiB is free. Process 2858670 has 3.90 GiB memory in use. Including non-PyTorch memory, this process has 11.87 GiB memory in use. Of the allocated memory 11.44 GiB is allocated by PyTorch, and 60.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 31\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     features_all\u001b[38;5;241m.\u001b[39mappend(features)\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/timm/models/vision_transformer.py:849\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 849\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/timm/models/vision_transformer.py:830\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    828\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 830\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/timm/models/vision_transformer.py:169\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 169\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    170\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))))\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/timm/models/vision_transformer.py:105\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m     x \u001b[38;5;241m=\u001b[39m attn \u001b[38;5;241m@\u001b[39m v\n\u001b[1;32m    104\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, N, C)\n\u001b[0;32m--> 105\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_drop(x)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/gpfs/scratch/nk4167/miniconda/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 704.00 KiB is free. Process 2858670 has 3.90 GiB memory in use. Including non-PyTorch memory, this process has 11.87 GiB memory in use. Of the allocated memory 11.44 GiB is allocated by PyTorch, and 60.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "id": "4eb138fb-d832-406d-8e7c-55c24fbc34b5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "execution_count": null,
      "outputs": [],
      "id": "e5bf602e-bd4e-4fa6-b8ab-4efdba7eefd3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}