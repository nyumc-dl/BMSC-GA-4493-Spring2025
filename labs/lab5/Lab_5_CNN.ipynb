{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 5: Introduction to Convolutional Layers\n",
    "\n",
    "\n",
    "\n",
    "The goal of this lab is to understand how to train a convolutional neural network using PyTorch. \n",
    "\n",
    "The dataset we will analyze will be a small section of the nih chest xrays dataset, found here: https://www.kaggle.com/nih-chest-xrays/sample . Please create a Kaggle account to download the data.\n",
    "\n",
    "The task at hand is to treat the dataset as a binary classification problem with image inputs. We propose to build a model that is a series of spatial convolutional layers, activation functions, and pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into the code, lets think for a bit about model selection. What are the necessary choices?\n",
    "\n",
    "Number of hidden layers?\n",
    "\n",
    "For each layer:\n",
    "\n",
    "    Number of filters?\n",
    "\n",
    "    Size of kernel?\n",
    "\n",
    "    Size of padding? (maybe (kernel - 1) / 2)\n",
    "    \n",
    "    Stride of layer?\n",
    "    \n",
    "    Activation after layer:\n",
    "    \n",
    "        Some type of relu, tanh, sigmoid?\n",
    "        \n",
    "        Maybe add Batch normalization before the activation function?\n",
    "    \n",
    "    Maybe a pooling layer instead of a convolutional layer to decrease spatial dimension?\n",
    "\n",
    "    Learning Rate?\n",
    "\n",
    "    Momentum parameters for optimizers such as ADAM?\n",
    "\n",
    "Other training techniques such as adding noise to input or hidden layers?\n",
    "\n",
    "Image specific techniques such as random rotations or blurring of the image?\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import common dependencies\n",
    "import torch\n",
    "import pandas as pd, numpy as np, matplotlib, matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset Selection\n",
    "\n",
    "### First, read in the sample labels which we will treat as y classes, and split into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path\n",
    "label_df = pd.read_csv('/home/ca3261/deep_learning/BMSC-GA-4493-Spring2025/labs/lab5/sample_labels.csv').iloc[:, :2]\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df['Disease']=(label_df['Finding Labels'] != 'No Finding').astype(int)\n",
    "print(label_df.head())\n",
    "num_rows = 1000\n",
    "label_df = label_df.iloc[:num_rows,:]\n",
    "\n",
    "# define train, val and test idx\n",
    "idx = np.arange(num_rows)\n",
    "np.random.shuffle(idx)\n",
    "train_size = 600\n",
    "val_size = 200\n",
    "test_size = 200\n",
    "train_idx = idx[:train_size]\n",
    "val_idx = idx[train_size:train_size+val_size]\n",
    "test_idx = idx[train_size+val_size:]\n",
    "\n",
    "# get train, val and test dataframes\n",
    "train_df = label_df.iloc[train_idx,:]\n",
    "val_df = label_df.iloc[val_idx,:]\n",
    "test_df = label_df.iloc[test_idx,:]\n",
    "\n",
    "# save the dataframes\n",
    "train_df.to_csv('train.csv', index = False)\n",
    "val_df.to_csv('val.csv', index = False)\n",
    "test_df.to_csv('test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We build dataloader to efficiently load the images and possibly also do some data augmentation on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xray_dataset(Dataset):\n",
    "    '''X-ray Dataset'''\n",
    "    def __init__(self, df_path, train = False):\n",
    "        self.df = pd.read_csv(df_path)\n",
    "        self.train = train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        file_name = self.df.iloc[idx,0]\n",
    "        label = self.df.iloc[idx,-1]\n",
    "        # change the path\n",
    "        img = Image.open('/home/ca3261/deep_learning/BMSC-GA-4493-Spring2025/labs/lab5/sample/sample/images/'+file_name)\n",
    "        img = img.resize((512, 512))\n",
    "        \n",
    "        if self.train:\n",
    "            # rotate the image (data augementation)\n",
    "            # question: why do we do this?\n",
    "            rand_num = np.random.random()\n",
    "            if rand_num > 0.7:\n",
    "                rot_angle = np.random.uniform(low = -10, high = 10)\n",
    "                img = img.rotate(rot_angle)\n",
    "                \n",
    "        img = np.asarray(img)\n",
    "        min_image = np.min(img)\n",
    "        max_image = np.max(img)\n",
    "        img = (img - min_image)/(max_image - min_image + 1e-4)\n",
    "        \n",
    "        img = torch.tensor(img).unsqueeze(0).float()\n",
    "        label = torch.tensor(label).long()\n",
    "        if img.dim() != 3:\n",
    "            img = img[:,:,:,0]\n",
    "        \n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the paths\n",
    "train_df_path = '/home/ca3261/deep_learning/BMSC-GA-4493-Spring2025/labs/lab5/train.csv'\n",
    "val_df_path = '/home/ca3261/deep_learning/BMSC-GA-4493-Spring2025/labs/lab5/val.csv'\n",
    "test_df_path = '/home/ca3261/deep_learning/BMSC-GA-4493-Spring2025/labs/lab5/test.csv'\n",
    "transformed_dataset = {'train': Xray_dataset(train_df_path, train = True),\n",
    "                       'validate':Xray_dataset(val_df_path),\n",
    "                       'test':Xray_dataset(test_df_path),\n",
    "                                          }\n",
    "bs = 4\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs,\n",
    "                        shuffle=True, num_workers=0) for x in ['train', 'validate','test']}\n",
    "data_sizes ={x: len(transformed_dataset[x]) for x in ['train', 'validate','test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = next(iter(dataloader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = sample[0][1].squeeze().numpy()\n",
    "plt.imshow(sample_img, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, scheduler, loss_fn, num_epochs = 10, verbose = False):\n",
    "    acc_dict = {'train':[],'validate':[]}\n",
    "    loss_dict = {'train':[],'validate':[]}\n",
    "    best_acc = 0\n",
    "    phases = ['train','validate']\n",
    "    since = time.time()\n",
    "    for i in range(num_epochs):\n",
    "        print('Epoch: {}/{}'.format(i, num_epochs-1))\n",
    "        print('-'*10)\n",
    "        for p in phases:\n",
    "            running_correct = 0\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            if p == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            for data in dataloader[p]:\n",
    "                optimizer.zero_grad()\n",
    "                image = data[0].to(device)\n",
    "                label = data[1].to(device)\n",
    "                output = model(image)\n",
    "                loss = loss_fn(output, label)\n",
    "                _, preds = torch.max(output, dim = 1)\n",
    "                num_imgs = image.size()[0]\n",
    "                running_correct += torch.sum(preds ==label).item()\n",
    "                running_loss += loss.item()*num_imgs\n",
    "                running_total += num_imgs\n",
    "                if p== 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            epoch_acc = float(running_correct/running_total)\n",
    "            epoch_loss = float(running_loss/running_total)\n",
    "            if verbose or (i%10 == 0):\n",
    "                print('Phase:{}, epoch loss: {:.4f} Acc: {:.4f}'.format(p, epoch_loss, epoch_acc))\n",
    "            \n",
    "            acc_dict[p].append(epoch_acc)\n",
    "            loss_dict[p].append(epoch_loss)\n",
    "            if p == 'validate':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = model.state_dict()\n",
    "            else:\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, acc_dict, loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader,loss_fn, phase = 'validate'):\n",
    "    model.eval()\n",
    "    running_correct = 0\n",
    "    running_loss = 0\n",
    "    running_total = 0\n",
    "    for data in dataloader[phase]:\n",
    "        image = data[0].to(device)\n",
    "        label = data[1].to(device)\n",
    "        output = model(image)\n",
    "        loss = loss_fn(output, label)\n",
    "        _, preds = torch.max(output,dim = 1)\n",
    "        num_imgs = image.size()[0]\n",
    "        running_correct += torch.sum(preds ==label).item()\n",
    "        running_loss += loss.item()*num_imgs\n",
    "        running_total += num_imgs\n",
    "    accuracy = float(running_correct/running_total)\n",
    "    loss = float(running_loss/running_total)\n",
    "    \n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_model(nn.Module):\n",
    "    def __init__(self, kernel_size = 3):\n",
    "        super(Conv_model,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,16,kernel_size, stride = 2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16,32,kernel_size, stride = 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32,64,kernel_size, padding = 1, stride = 3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64,128,kernel_size, padding = 1, stride = 3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128,256,kernel_size)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear = nn.Linear(256, 2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.relu5(self.conv5(x))\n",
    "        x = self.avg(x)\n",
    "        x = self.linear(x.view(-1,256))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cel = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "lambda_func = lambda epoch: 0.5 ** epoch\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, acc_dict, loss_dict = train_model(model, dataloader, optimizer, scheduler, cel, num_epochs=10, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Now lets evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, loss = evaluate_model(model, dataloader,cel,phase = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we change the architecture a bit by adding [Batch Norms](https://arxiv.org/abs/1502.03167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_model_bn(nn.Module):\n",
    "    def __init__(self, kernel_size = 3):\n",
    "        super(Conv_model_bn,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,16,kernel_size, stride = 2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16,32,kernel_size, stride = 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32,64,kernel_size, padding = 1, stride = 3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64,128,kernel_size, padding = 1, stride = 3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.conv5 = nn.Conv2d(128,256,kernel_size)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear = nn.Linear(256, 2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        x = self.relu3(self.bn3(self.conv3(x)))\n",
    "        x = self.relu4(self.bn4(self.conv4(x)))\n",
    "        x = self.relu5(self.bn5(self.conv5(x)))\n",
    "        x = self.avg(x)\n",
    "        x = self.linear(x.view(-1,256))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv_model_bn().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cel = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "# lambda_func = lambda epoch: 0.5 ** epoch\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, acc_dict, loss_dict = train_model(model, dataloader, optimizer, None, cel, num_epochs=10, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, loss = evaluate_model(model, dataloader,cel,phase = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a proven CNN architecture - Inception V3\n",
    "\n",
    "Inception v3 model architecture from [“Rethinking the Inception Architecture for Computer Vision”](https://arxiv.org/abs/1512.00567).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from https://pytorch.org/docs/stable/_modules/torchvision/models/inception.html#inception_v3\n",
    "from __future__ import division\n",
    "\n",
    "from collections import namedtuple\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.jit.annotations import Optional\n",
    "from torch import Tensor\n",
    "\n",
    "InceptionOutputs = namedtuple('InceptionOutputs', ['logits', 'aux_logits'])\n",
    "InceptionOutputs.__annotations__ = {'logits': torch.Tensor, 'aux_logits': Optional[torch.Tensor]}\n",
    "\n",
    "# script annotations failed with _GoogleNetOutputs = namedtuple ...\n",
    "# _InceptionOutputs set here for backwards compat\n",
    "_InceptionOutputs = InceptionOutputs\n",
    "\n",
    "\n",
    "class Inception3(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2, aux_logits=False, transform_input=False,\n",
    "                 inception_blocks=None):\n",
    "        super(Inception3, self).__init__()\n",
    "        if inception_blocks is None:\n",
    "            inception_blocks = [\n",
    "                BasicConv2d, InceptionA, InceptionB, InceptionC,\n",
    "                InceptionD, InceptionE, InceptionAux\n",
    "            ]\n",
    "        assert len(inception_blocks) == 7\n",
    "        conv_block = inception_blocks[0]\n",
    "        inception_a = inception_blocks[1]\n",
    "        inception_b = inception_blocks[2]\n",
    "        inception_c = inception_blocks[3]\n",
    "        inception_d = inception_blocks[4]\n",
    "        inception_e = inception_blocks[5]\n",
    "        inception_aux = inception_blocks[6]\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "        self.Conv2d_1a_3x3 = conv_block(1, 32, kernel_size=3, stride=2)\n",
    "        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)\n",
    "        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)\n",
    "        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)\n",
    "        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)\n",
    "        self.Mixed_5b = inception_a(192, pool_features=32)\n",
    "        self.Mixed_5c = inception_a(256, pool_features=64)\n",
    "        self.Mixed_5d = inception_a(288, pool_features=64)\n",
    "        self.Mixed_6a = inception_b(288)\n",
    "        self.Mixed_6b = inception_c(768, channels_7x7=128)\n",
    "        self.Mixed_6c = inception_c(768, channels_7x7=160)\n",
    "        self.Mixed_6d = inception_c(768, channels_7x7=160)\n",
    "        self.Mixed_6e = inception_c(768, channels_7x7=192)\n",
    "        if aux_logits:\n",
    "            self.AuxLogits = inception_aux(768, num_classes)\n",
    "        self.Mixed_7a = inception_d(768)\n",
    "        self.Mixed_7b = inception_e(1280)\n",
    "        self.Mixed_7c = inception_e(2048)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n",
    "                X = stats.truncnorm(-2, 2, scale=stddev)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _transform_input(self, x):\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "        return x\n",
    "\n",
    "    def _forward(self, x):\n",
    "        # N x 3 x 299 x 299\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # N x 32 x 149 x 149\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # N x 32 x 147 x 147\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # N x 64 x 147 x 147\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # N x 64 x 73 x 73\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # N x 80 x 73 x 73\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # N x 192 x 71 x 71\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # N x 192 x 35 x 35\n",
    "        x = self.Mixed_5b(x)\n",
    "        # N x 256 x 35 x 35\n",
    "        x = self.Mixed_5c(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_5d(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_6a(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6b(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6c(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6d(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6e(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if aux_defined:\n",
    "            aux = self.AuxLogits(x)\n",
    "        else:\n",
    "            aux = None\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_7a(x)\n",
    "        # N x 1280 x 8 x 8\n",
    "        x = self.Mixed_7b(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        x = self.Mixed_7c(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        # Adaptive average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 2048\n",
    "        x = self.fc(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        return x, aux\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def eager_outputs(self, x, aux):\n",
    "        # type: (Tensor, Optional[Tensor]) -> InceptionOutputs\n",
    "        if self.training and self.aux_logits:\n",
    "            return InceptionOutputs(x, aux)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._transform_input(x)\n",
    "        x, aux = self._forward(x)\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if torch.jit.is_scripting():\n",
    "            if not aux_defined:\n",
    "                warnings.warn(\"Scripted Inception3 always returns Inception3 Tuple\")\n",
    "            return InceptionOutputs(x, aux)\n",
    "        else:\n",
    "            return self.eager_outputs(x, aux)\n",
    "\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, pool_features, conv_block=None):\n",
    "        super(InceptionA, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 64, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = conv_block(in_channels, 48, kernel_size=1)\n",
    "        self.branch5x5_2 = conv_block(48, 64, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionB(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, conv_block=None):\n",
    "        super(InceptionB, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch3x3 = conv_block(in_channels, 384, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, stride=2)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "\n",
    "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionC(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, channels_7x7, conv_block=None):\n",
    "        super(InceptionC, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "        c7 = channels_7x7\n",
    "        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7_3 = conv_block(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "\n",
    "        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_5 = conv_block(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch7x7 = self.branch7x7_1(x)\n",
    "        branch7x7 = self.branch7x7_2(branch7x7)\n",
    "        branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionD(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, conv_block=None):\n",
    "        super(InceptionD, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch3x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "        self.branch3x3_2 = conv_block(192, 320, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch7x7x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "        self.branch7x7x3_2 = conv_block(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7x3_3 = conv_block(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7x3_4 = conv_block(192, 192, kernel_size=3, stride=2)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\n",
    "\n",
    "        branch7x7x3 = self.branch7x7x3_1(x)\n",
    "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionE(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, conv_block=None):\n",
    "        super(InceptionE, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 320, kernel_size=1)\n",
    "\n",
    "        self.branch3x3_1 = conv_block(in_channels, 384, kernel_size=1)\n",
    "        self.branch3x3_2a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3_2b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 448, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(448, 384, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3dbl_3b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, conv_block=None):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.conv0 = conv_block(in_channels, 128, kernel_size=1)\n",
    "        self.conv1 = conv_block(128, 768, kernel_size=5)\n",
    "        self.conv1.stddev = 0.01\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        self.fc.stddev = 0.001\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N x 768 x 17 x 17\n",
    "        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n",
    "        # N x 768 x 5 x 5\n",
    "        x = self.conv0(x)\n",
    "        # N x 128 x 5 x 5\n",
    "        x = self.conv1(x)\n",
    "        # N x 768 x 1 x 1\n",
    "        # Adaptive average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # N x 768 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 768\n",
    "        x = self.fc(x)\n",
    "        # N x 1000\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Inception3().to(device)\n",
    "cel = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "lambda_func = lambda epoch: 0.8 ** epoch\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, acc_dict, loss_dict = train_model(model, dataloader, optimizer, scheduler, cel, num_epochs=10, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, loss = evaluate_model(model, dataloader,cel,phase = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reading\n",
    "\n",
    "https://cs231n.github.io/convolutional-networks/ - ConvNets for Visual Recognition\n",
    "https://arxiv.org/abs/1502.03167 - Batch Normalization\n",
    "https://arxiv.org/abs/1512.00567 - InceptionV3 architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
