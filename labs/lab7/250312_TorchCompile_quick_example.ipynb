{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0908332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c14e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(32 * 7 * 7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d402f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a069da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Use a local data directory instead of hard-coded /mnt/data\n",
    "data_root = Path(\"MNIST\")\n",
    "data_root.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(str(data_root))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa08e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already there\n"
     ]
    }
   ],
   "source": [
    "# MNIST directory is created in data_root above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10aca01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST, this is just an example\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "trainset = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2,drop_last=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='.', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91988a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Naive\" model\n",
    "model = SimpleCNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae6bf0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "compiled_model = torch.compile(model,backend='cudagraphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1df26a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, loss\n",
    "optimizer = optim.Adam(compiled_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7280ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def train(model, trainloader, optimizer, criterion, epochs=1):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_time = time.time()-start_time\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}, time: {epoch_time:.2f}s\")\n",
    "\n",
    "def evaluate(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].cuda(), data[1].cuda()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c235140",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c18f8202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daced505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training compiled model:\n",
      "Epoch 1, Loss: 0.1820743108931591, time: 5.71s\n",
      "Epoch 2, Loss: 0.05601658393766059, time: 5.73s\n",
      "Epoch 3, Loss: 0.04027422228071435, time: 5.90s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolas/miniconda3/lib/python3.9/site-packages/torch/_inductor/cudagraph_trees.py:2385: UserWarning: Unable to hit fast path of CUDAGraphs because of pending, uninvoked backwards. Consider running with torch.no_grad() or using torch.compiler.cudagraph_mark_step_begin() before each model invocation\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.83%\n",
      "18.16615915298462\n",
      "Training uncompiled model:\n",
      "Epoch 1, Loss: 0.1786678190191009, time: 5.85s\n",
      "Epoch 2, Loss: 0.05807021551740901, time: 5.63s\n",
      "Epoch 3, Loss: 0.04264528390537975, time: 5.82s\n",
      "Accuracy: 98.59%\n",
      "18.048738956451416\n"
     ]
    }
   ],
   "source": [
    "# Train the compiled model\n",
    "print(\"Training compiled model (warm-up for compilation):\")\n",
    "train(compiled_model, trainloader, optimizer, criterion, epochs=1)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "start_time = time.time()\n",
    "train(compiled_model, trainloader, optimizer, criterion, epochs=3)\n",
    "evaluate(compiled_model, testloader)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time - start_time)\n",
    "\n",
    "#Train the uncompiled model.\n",
    "start_time = time.time()\n",
    "\n",
    "model2 = SimpleCNN().cuda()\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "print(\"Training uncompiled model:\")\n",
    "train(model2, trainloader, optimizer2, criterion, epochs=3)\n",
    "evaluate(model2, testloader)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "print(end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ffe84b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 100),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5fb006fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolas/miniconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128, 100])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled training time: 0.3453 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTOTUNE addmm(128x1, 128x32, 32x1)\n",
      "  triton_mm_702 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=2\n",
      "  triton_mm_703 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2\n",
      "  triton_mm_704 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4\n",
      "  triton_mm_706 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_mm_707 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_mm_708 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4\n",
      "  triton_mm_705 0.0050 ms 82.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_mm_701 0.0051 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2\n",
      "  triton_mm_709 0.0051 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_mm_710 0.0051 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=16, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 1.4330 seconds and 0.0010 seconds precompiling for 13 choices\n",
      "/home/nikolas/miniconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([128, 100])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] Triton compilation failed: Placeholder.DESCRIPTIVE_NAME\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] def triton_mm(arg_A, arg_B, out_ptr0):\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     GROUP_M : tl.constexpr = 8\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     EVEN_K : tl.constexpr = False\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     ALLOW_TF32 : tl.constexpr = True\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     ACC_TYPE : tl.constexpr = tl.float32\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     B_PROLOGUE_CAST_TYPE : tl.constexpr = None\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     BLOCK_M : tl.constexpr = 64\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     BLOCK_N : tl.constexpr = 32\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     BLOCK_K : tl.constexpr = 16\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     A = arg_A\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     B = arg_B\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] \n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     M = 128\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     N = 32\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     K = 1\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     if M * N == 0:\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]         # early exit due to zero-size input(s)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]         return\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     stride_am = 1\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     stride_ak = 1\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     stride_bk = 32\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     stride_bn = 1\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] \n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     # based on triton.ops.matmul\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     pid = tl.program_id(0)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     grid_m = (M + BLOCK_M - 1) // BLOCK_M\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     grid_n = (N + BLOCK_N - 1) // BLOCK_N\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] \n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     # re-order program ID for better L2 performance\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     width = GROUP_M * grid_n\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     group_id = pid // width\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     pid_m = group_id * GROUP_M + (pid % group_size)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     pid_n = (pid % width) // (group_size)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] \n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]         ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     else:\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]         ram = rm % M\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]         rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     else:\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]         rbn = rn % N\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     rk = tl.arange(0, BLOCK_K)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] \n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     for k in range(K, 0, -BLOCK_K):\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]         if EVEN_K:\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]             a = tl.load(A)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]             b = tl.load(B)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]         else:\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]             a = tl.load(A, mask=rk[None, :] < k, other=0.)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]             b = tl.load(B, mask=rk[:, None] < k, other=0.)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]         if B_PROLOGUE_CAST_TYPE is not None:\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]             b = b.to(B_PROLOGUE_CAST_TYPE)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]         acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]         A += BLOCK_K * stride_ak\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]         B += BLOCK_K * stride_bk\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] \n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     # rematerialize rm and rn to save registers\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     idx_m = rm[:, None]\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     idx_n = rn[None, :]\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     mask = (idx_m < M) & (idx_n < N)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] \n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     # inductor generates a suffix\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     xindex = idx_n + 32*idx_m\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] \n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] metadata: {'signature': {'arg_A': '*fp32', 'arg_B': '*fp32', 'out_ptr0': '*fp32'}, 'device': 0, 'constants': {}, 'configs': [AttrsDescriptor.from_dict({'arg_properties': {'tt.divisibility': (0, 1, 2), 'tt.equal_to': ()}, 'cls': 'AttrsDescriptor'})], 'device_type': 'cuda', 'num_warps': 8, 'num_stages': 5, 'debug': True, 'cc': 86}\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] Traceback (most recent call last):\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]   File \"/home/nikolas/miniconda3/lib/python3.9/site-packages/triton/backends/nvidia/compiler.py\", line 356, in make_cubin\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]   File \"/home/nikolas/miniconda3/lib/python3.9/subprocess.py\", line 528, in run\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     raise CalledProcessError(retcode, process.args,\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] subprocess.CalledProcessError: Command '['/home/nikolas/miniconda3/lib/python3.9/site-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_86', '/tmp/tmpyb375rly.ptx', '-o', '/tmp/tmpyb375rly.ptx.o']' died with <Signals.SIGINT: 2>.\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] \n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] During handling of the above exception, another exception occurred:\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] \n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] Traceback (most recent call last):\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]   File \"/home/nikolas/miniconda3/lib/python3.9/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 511, in _precompile_config\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     binary = triton.compile(*compile_args, **compile_kwargs)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]   File \"/home/nikolas/miniconda3/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 279, in compile\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     next_module = compile_ir(module, metadata)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]   File \"/home/nikolas/miniconda3/lib/python3.9/site-packages/triton/backends/nvidia/compiler.py\", line 389, in <lambda>\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     stages[\"cubin\"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.capability)\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]   File \"/home/nikolas/miniconda3/lib/python3.9/site-packages/triton/backends/nvidia/compiler.py\", line 374, in make_cubin\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513]     raise RuntimeError(f'{error}\\n'\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] RuntimeError: `ptxas` failed with error code -2\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] `ptxas` stderr:\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] \n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] Repro command: /home/nikolas/miniconda3/lib/python3.9/site-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_86 /tmp/tmpyb375rly.ptx -o /tmp/tmpyb375rly.ptx.o\n",
      "E0313 08:33:10.300372 160764 site-packages/torch/_inductor/runtime/triton_heuristics.py:513] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0313 08:33:12.997363 160764 site-packages/torch/_inductor/select_algorithm.py:1477] [2/0] Exception `ptxas` failed with error code -2\n",
      "E0313 08:33:12.997363 160764 site-packages/torch/_inductor/select_algorithm.py:1477] [2/0] `ptxas` stderr:\n",
      "E0313 08:33:12.997363 160764 site-packages/torch/_inductor/select_algorithm.py:1477] [2/0] \n",
      "E0313 08:33:12.997363 160764 site-packages/torch/_inductor/select_algorithm.py:1477] [2/0] Repro command: /home/nikolas/miniconda3/lib/python3.9/site-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_86 /tmp/tmpyb375rly.ptx -o /tmp/tmpyb375rly.ptx.o\n",
      "E0313 08:33:12.997363 160764 site-packages/torch/_inductor/select_algorithm.py:1477] [2/0]  for benchmark choice TritonTemplateCaller(/tmp/torchinductor_nikolas/xt/cxtpo4gcqqa66qioxrafdkpahmf5cogsaubxphjbsksltk25rney.py, ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8)\n",
      "AUTOTUNE mm(128x1, 1x32)\n",
      "  mm 0.0041 ms 100.0% \n",
      "  triton_mm_713 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=1, num_warps=2\n",
      "  triton_mm_714 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_mm_715 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4\n",
      "  triton_mm_716 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8\n",
      "  triton_mm_717 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4\n",
      "  triton_mm_718 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_mm_719 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8\n",
      "  triton_mm_720 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_mm_721 0.0041 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 1.8231 seconds and 0.0027 seconds precompiling for 15 choices\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUncompiled training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muncompiled_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Warmup\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m compiled_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiled_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiled_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompiled training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiled_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeedup: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muncompiled_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mcompiled_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[107], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, inputs, targets, epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     24\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Random data\n",
    "batch_size = 128\n",
    "input_size = 1024\n",
    "output_size = 100\n",
    "inputs = torch.randn(batch_size, input_size).cuda()\n",
    "targets = torch.randn(batch_size, output_size).cuda()\n",
    "\n",
    "# Models\n",
    "model = ComplexModel().cuda()\n",
    "compiled_model = torch.compile(model,mode=\"max-autotune\",backend='inductor')\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "compiled_optimizer = optim.Adam(compiled_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train(model, optimizer, criterion, inputs, targets, epochs=10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Train uncompiled model\n",
    "uncompiled_time = train(model, optimizer, criterion, inputs, targets)\n",
    "print(f\"Uncompiled training time: {uncompiled_time:.4f} seconds\")\n",
    "\n",
    "# Warmup\n",
    "compiled_time = train(compiled_model, compiled_optimizer, criterion, inputs, targets)\n",
    "print(f\"Compiled training time: {compiled_time:.4f} seconds\")\n",
    "\n",
    "print(f\"Speedup: {uncompiled_time / compiled_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ccf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train uncompiled model\n",
    "uncompiled_time = train(model, optimizer, criterion, inputs, targets)\n",
    "print(f\"Uncompiled training time: {uncompiled_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e290c347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled training time: 0.0453 seconds\n",
      "Speedup: 1.72x\n"
     ]
    }
   ],
   "source": [
    "# Post JIT, damn this took a while.\n",
    "compiled_time = train(compiled_model, compiled_optimizer, criterion, inputs, targets)\n",
    "print(f\"Compiled training time: {compiled_time:.4f} seconds\")\n",
    "\n",
    "print(f\"Speedup: {uncompiled_time / compiled_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3918f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56bc4699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9bef0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "79054cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2b6cd477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([153600])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4f17dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "569c0483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random data\n",
    "batch_size = 1024*150\n",
    "input_size = 256\n",
    "output_size = 10\n",
    "inputs = torch.randn(batch_size, input_size).cuda()\n",
    "targets = torch.tensor(np.expand_dims([1 if x else 0  for x in inputs.sum(axis=1) > torch.mean(inputs.sum(axis=1))],axis=1))\n",
    "targets = targets.to(dtype=torch.float).cuda()\n",
    "# Create TensorDataset\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "\n",
    "# Create DataLoader\n",
    "trainloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# Helpers\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(256, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    \n",
    "def get_memory_usage():\n",
    "    pid = os.getpid()\n",
    "    process = psutil.Process(pid)\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / (1024 * 1024)  # Return memory usage in MB\n",
    "\n",
    "def get_cuda_memory_usage():\n",
    "    allocated_memory = torch.cuda.memory_allocated() / (1024 * 1024)  # MB\n",
    "    cached_memory = torch.cuda.memory_reserved() / (1024 * 1024) #MB\n",
    "    return allocated_memory, cached_memory\n",
    "\n",
    "def train(model, trainloader, optimizer, criterion, epochs=1):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "            optimizer.zero_grad()\n",
    "          \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_time = time.time()-start_time\n",
    "        print(f\"Outputs dtype: {outputs.dtype}\")\n",
    "        allocated_memory, cached_memory = get_cuda_memory_usage()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}, time: {epoch_time:.2f}s, Allocated Memory: {allocated_memory:.2f} MB, Cached Memory: {cached_memory:.2f} MB\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "def train2(model, trainloader, optimizer, criterion,scaler, epochs=1):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item()\n",
    "        epoch_time = time.time()-start_time\n",
    "        print(f\"Outputs dtype: {outputs.dtype}\")\n",
    "        allocated_memory, cached_memory = get_cuda_memory_usage()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}, time: {epoch_time:.2f}s, Allocated Memory: {allocated_memory:.2f} MB, Cached Memory: {cached_memory:.2f} MB\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fc52f21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "82e1c433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with autocast:\n",
      "Outputs dtype: torch.float16\n",
      "Epoch 1, Loss: 0.0, time: 2.01s, Allocated Memory: 4578.44 MB, Cached Memory: 9780.00 MB\n",
      "Outputs dtype: torch.float16\n",
      "Epoch 2, Loss: 0.0, time: 2.00s, Allocated Memory: 4578.44 MB, Cached Memory: 9932.00 MB\n",
      "Outputs dtype: torch.float16\n",
      "Epoch 3, Loss: 0.0, time: 2.00s, Allocated Memory: 4578.44 MB, Cached Memory: 9932.00 MB\n",
      "Outputs dtype: torch.float16\n",
      "Epoch 4, Loss: 0.0, time: 2.01s, Allocated Memory: 4578.44 MB, Cached Memory: 9932.00 MB\n",
      "Outputs dtype: torch.float16\n",
      "Epoch 5, Loss: 0.0, time: 1.99s, Allocated Memory: 4578.44 MB, Cached Memory: 9932.00 MB\n"
     ]
    }
   ],
   "source": [
    "model = ComplexModel().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scaler = GradScaler()\n",
    "print(\"Training with autocast:\")\n",
    "train2(model, trainloader, optimizer, criterion, scaler, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "742580d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d817bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training without autocast:\n",
      "Outputs dtype: torch.float32\n",
      "Epoch 1, Loss: 0.44498562812805176, time: 2.31s, Allocated Memory: 4579.57 MB, Cached Memory: 14580.00 MB\n",
      "Outputs dtype: torch.float32\n",
      "Epoch 2, Loss: 0.4326412081718445, time: 2.30s, Allocated Memory: 4579.57 MB, Cached Memory: 14732.00 MB\n",
      "Outputs dtype: torch.float32\n",
      "Epoch 3, Loss: 0.3801073133945465, time: 2.29s, Allocated Memory: 4579.57 MB, Cached Memory: 14730.00 MB\n",
      "Outputs dtype: torch.float32\n",
      "Epoch 4, Loss: 0.24810852110385895, time: 2.28s, Allocated Memory: 4579.57 MB, Cached Memory: 14732.00 MB\n",
      "Outputs dtype: torch.float32\n",
      "Epoch 5, Loss: 0.23171819746494293, time: 2.29s, Allocated Memory: 4579.57 MB, Cached Memory: 14732.00 MB\n"
     ]
    }
   ],
   "source": [
    "model = ComplexModel().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print(\"Training without autocast:\")\n",
    "train(model, trainloader, optimizer, criterion, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3049510a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "58d34546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from datasets import load_dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def setup_process(rank, world_size):\n",
    "    # Initialize the distributed process group (using NCCL backend for GPUs)\n",
    "    dist.init_process_group(\n",
    "        backend=\"nccl\",\n",
    "        init_method=\"env://\",\n",
    "        world_size=world_size,\n",
    "        rank=rank,\n",
    "    )\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def ddp_training(rank, world_size):\n",
    "    setup_process(rank, world_size)\n",
    "    device = torch.device(f\"cuda:{rank}\")\n",
    "    \n",
    "    # Use a clinical transformer model (Bio_ClinicalBERT) for medical text\n",
    "    model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "    model.to(device)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    # Load the MedNLI dataset (a medical natural language inference dataset)\n",
    "    dataset = load_dataset(\"mednli\", split=\"train\")\n",
    "    \n",
    "    # Initialize tokenizer from the same model\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize the dataset: combine \"premise\" and \"hypothesis\" with truncation and padding.\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"premise\"], example[\"hypothesis\"],\n",
    "                         truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    dataset = dataset.map(tokenize_function, batched=True)\n",
    "    # Set the format for PyTorch tensors, keeping input_ids, attention_mask, and label.\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    # Use DistributedSampler so each process gets a unique subset of data.\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, sampler=sampler)\n",
    "    \n",
    "    # Set up the optimizer.\n",
    "    optimizer = AdamW(ddp_model.parameters(), lr=2e-5)\n",
    "    \n",
    "    ddp_model.train()\n",
    "    num_epochs = 2  # For demonstration, we use a small number of epochs.\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set epoch for sampler to have proper shuffling\n",
    "        sampler.set_epoch(epoch)\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            outputs = ddp_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Only rank 0 logs to avoid duplicate output.\n",
    "            if rank == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6da3d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/nikolas/miniconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/nikolas/miniconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'ddp_training' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 0 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMASTER_PORT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m12355\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Spawn one process per GPU\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mddp_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:340\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    334\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n\u001b[1;32m    339\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspawn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:296\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:204\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout, grace_period)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    197\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, name),\n\u001b[1;32m    198\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m             signal_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    202\u001b[0m         )\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, exitcode),\n\u001b[1;32m    206\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    207\u001b[0m             error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    208\u001b[0m             exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_files[error_index], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[1;32m    212\u001b[0m     original_trace \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fh)\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "# Use the number of available GPUs for distributed training\n",
    "world_size = torch.cuda.device_count()\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "\n",
    "# Spawn one process per GPU (skip by default in notebooks)\n",
    "if os.environ.get(\"RUN_DDP\", \"0\") == \"1\":\n",
    "    mp.spawn(ddp_training, args=(world_size,), nprocs=world_size, join=True)\n",
    "else:\n",
    "    print(\"Skipping DDP example (set RUN_DDP=1 to run).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff915b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
