{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e12826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from xformers.ops import memory_efficient_attention  # Flash Attention\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf1e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0368499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset (Ensure the file is downloaded from Kaggle: https://www.kaggle.com/tboyle10/medicaltranscriptions)\n",
    "df = pd.read_csv(\"mtsamples.csv\")\n",
    "\n",
    "# Select relevant columns (assuming 'description' as text and 'medical_specialty' as labels)\n",
    "df = df[['transcription', 'medical_specialty']].dropna()\n",
    "\n",
    "# Reduce the number of categories for a binary classification task\n",
    "df['LABEL'] = df['medical_specialty'].apply(lambda x: 1 if x == ' Surgery' else 0)\n",
    "\n",
    "# Splitting dataset into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['transcription'].values, df['LABEL'].values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Define dataset parameters\n",
    "max_len = 512\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset = MedicalDataset(train_texts, train_labels, tokenizer, max_len)\n",
    "val_dataset = MedicalDataset(val_texts, val_labels, tokenizer, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5bc5ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LABEL\n",
       "0    3878\n",
       "1    1088\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['LABEL'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b40ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3972"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7120660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee123eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 10507,  1024,  ...,  2150,  2062,   102],\n",
       "         [  101,  3653, 25918,  ...,  1996,  5923,   102],\n",
       "         [  101,  3653, 25918,  ...,  4013, 26745,   102],\n",
       "         ...,\n",
       "         [  101,  2381,  1997,  ...,  1011,  4097,   102],\n",
       "         [  101, 25086, 11655,  ...,     0,     0,     0],\n",
       "         [  101,  3653, 25918,  ...,  1996,  5776,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels': tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "         1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1e0d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_max = 0\n",
    "for i in train_loader:\n",
    "    temp_max = i['input_ids'].max().detach().cpu().numpy()\n",
    "    if temp_max > curr_max:\n",
    "        curr_max = temp_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9915e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in val_loader:\n",
    "    temp_max = i['input_ids'].max().detach().cpu().numpy()\n",
    "    if temp_max > curr_max:\n",
    "        curr_max = temp_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad6dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = curr_max + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13253a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # Cool thread to visualize it: https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model, device=device)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, device=device).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:pe[:, 1::2].shape[1]])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  \n",
    "        self.register_buffer('pe', pe)\n",
    "    #Layer that adds the encoding.\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8dbc0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashAttentionLayer(nn.Module):\n",
    "    # Quick flash attention implementation\n",
    "    def __init__(self, embed_dim, nhead, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % nhead == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        self.nhead = nhead\n",
    "        self.head_dim = embed_dim // nhead  # Get head dimensions\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn_proj_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_proj_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_proj_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, C = x.shape  # Batch, Sequence Length, Embedding Dim\n",
    "        # Project linearly for Q, K, V\n",
    "        q = self.attn_proj_q(x).view(B, L, self.nhead, self.head_dim).transpose(1, 2)  # (B, nh, L, head_dim)\n",
    "        k = self.attn_proj_k(x).view(B, L, self.nhead, self.head_dim).transpose(1, 2)\n",
    "        v = self.attn_proj_v(x).view(B, L, self.nhead, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Apply Flash Attention\n",
    "        attn_output = memory_efficient_attention(q, k, v)  # (B, nh, L, head_dim)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(B, L, C)  # Reshape back to (B, L, C)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9cf8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Important to note the many parameters here, that's what we're optimizing\n",
    "class SmallFlashTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, nhead, num_layers, dropout, num_classes=2):\n",
    "        super().__init__()\n",
    "        #Make embedding and encoding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
    "        # Note: You can stack as many attention layers as you want\n",
    "        self.layers = nn.ModuleList([\n",
    "            FlashAttentionLayer(embed_dim, nhead, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        # Linear head\n",
    "        # Add extra layers for better feature extraction\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim, embed_dim * 2)\n",
    "        self.activation = nn.GELU()\n",
    "        self.fc2 = nn.Linear(embed_dim * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "        # Note how init weights is applied\n",
    "        self.apply(self._init_weights)\n",
    "    # Note the model initialization and weights\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "            \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.float()\n",
    "        # Note the attention mask that is used to only pay attention \"backwards\" through the text.\n",
    "        #Not necessary for nucleic acids\n",
    "        if attention_mask is not None:\n",
    "            x = x * attention_mask.unsqueeze(-1)\n",
    "            # Mean pooling with mask - Note the representation used - Mean token representation here:\n",
    "            x = x.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True).clamp(min=1e-9)\n",
    "            # Last token representation version\n",
    "            #x = x[:,-1,:] / attention_mask.sum(dim=1, keepdim=True).clamp(min=1e-9)\n",
    "        else:\n",
    "            x = x.mean(dim=1)\n",
    "            #x = x[:,-1,:]\n",
    "        # Note the Fully Connected head built on top of the attention layers\n",
    "        x = self.norm(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73df1988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You need to generate a training function, it's also good practice to do so.\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask)\n",
    "        # Debugging output shapes\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        # Note the gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        scheduler.step()\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Debugging output shapes\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2c08505",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-11 13:45:26,047] A new study created in memory with name: no-name-d4ebe030-acfb-4a2b-9b14-27a7d93ab06e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.23138832997987926\n",
      "1\n",
      "0.4798792756539235\n",
      "2\n",
      "0.682092555331992\n",
      "3\n",
      "0.6438631790744467\n",
      "4\n",
      "0.6680080482897385\n",
      "5\n",
      "0.6348088531187123\n",
      "6\n",
      "0.6519114688128773\n",
      "7\n",
      "0.635814889336016\n",
      "8\n",
      "0.635814889336016\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-11 13:47:59,079] Trial 0 finished with value: 0.601291082154817 and parameters: {'embed_dim': 128, 'nhead': 1, 'num_layers': 2, 'dropout': 0.4337938793442445, 'lr': 0.0038079254590203406, 'batch_size': 128}. Best is trial 0 with value: 0.601291082154817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.635814889336016\n",
      "0\n",
      "0.23138832997987926\n",
      "1\n",
      "0.23138832997987926\n",
      "2\n",
      "0.23138832997987926\n",
      "3\n",
      "0.727364185110664\n",
      "4\n",
      "0.635814889336016\n",
      "5\n",
      "0.6579476861167002\n",
      "6\n",
      "0.7142857142857143\n",
      "7\n",
      "0.676056338028169\n",
      "8\n",
      "0.693158953722334\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-11 13:50:06,015] Trial 1 finished with value: 0.577077158739869 and parameters: {'embed_dim': 64, 'nhead': 4, 'num_layers': 1, 'dropout': 0.17258195662547351, 'lr': 0.002490214165099875, 'batch_size': 512}. Best is trial 1 with value: 0.577077158739869.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693158953722334\n",
      "0\n",
      "0.23138832997987926\n",
      "1\n",
      "0.6851106639839034\n",
      "2\n",
      "0.7092555331991952\n",
      "3\n",
      "0.6780684104627767\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-11 13:51:03,841] Trial 2 failed with parameters: {'embed_dim': 64, 'nhead': 4, 'num_layers': 1, 'dropout': 0.4615023374789826, 'lr': 0.006268787743472726, 'batch_size': 256} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nikolas/miniconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_12506/1989297933.py\", line 37, in objective\n",
      "    train_loss = train_epoch(model, train_loader, optimizer, scheduler,criterion)\n",
      "  File \"/tmp/ipykernel_12506/774603689.py\", line 5, in train_epoch\n",
      "    for batch in dataloader:\n",
      "  File \"/home/nikolas/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/nikolas/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 764, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/nikolas/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/nikolas/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/tmp/ipykernel_12506/2556087499.py\", line 33, in __getitem__\n",
      "    encoding = self.tokenizer.encode_plus(\n",
      "  File \"/home/nikolas/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3063, in encode_plus\n",
      "    return self._encode_plus(\n",
      "  File \"/home/nikolas/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 613, in _encode_plus\n",
      "    batched_output = self._batch_encode_plus(\n",
      "  File \"/home/nikolas/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 539, in _batch_encode_plus\n",
      "    encodings = self._tokenizer.encode_batch(\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-11 13:51:03,843] Trial 2 failed with value None.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# For optuna, it's good to generate a function that runs all your suggestions through.\n",
    "def objective(trial):\n",
    "    #This is what we'll test, but it could be anything, HEHE!\n",
    "    embed_dim = trial.suggest_categorical(\"embed_dim\", [32, 64, 128])\n",
    "    nhead = trial.suggest_categorical(\"nhead\", [1, 2, 4])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 2)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [128,256,512])\n",
    "    # Make the transformer\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    model = SmallFlashTransformer(vocab_size, embed_dim, nhead, num_layers, dropout).to(device)\n",
    "    #NOTE this: Weighted classes for CrossEntropy loss\n",
    "    class_counts = df['LABEL'].value_counts().to_list()\n",
    "    weights = [1.0 / count for count in class_counts]\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    #Note the weight and label smoothing\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "    #Note the difference between Adam and Adam Weight Decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    #Note difference between StepLR and OneCycleLR\n",
    "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1) \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=lr,\n",
    "            epochs=10,\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            pct_start=0.1\n",
    "        )\n",
    "    # Generate data loaders\n",
    "    # Set how many epocs you want\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler,criterion)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "        print(val_acc)\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_loss\n",
    "# Do you want to minimize or maximize the objective (val_loss?)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "# How many trials? optuna has several optimization algoirthms included\n",
    "study.optimize(objective, n_trials=20, timeout=600)\n",
    "#https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html Oh god, there's so many of them.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Validation Loss:\", trial.value)\n",
    "print(\"  Best hyperparameters:\", trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecc2265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of SmallFlashTransformer(\n",
      "  (embedding): Embedding(29653, 512)\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x FlashAttentionLayer(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_proj_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (attn_proj_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (attn_proj_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (feedforward): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (activation): GELU(approximate='none')\n",
      "  (fc2): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")>\n",
      "Total number of parameters: 40929794\n"
     ]
    }
   ],
   "source": [
    "model = SmallFlashTransformer(vocab_size, 512, 8, 8, 0.3).to(device)\n",
    "print(model.parameters)\n",
    "total_params = 0\n",
    "for param in model.parameters():\n",
    "    total_params += param.numel()\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd6959e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from muon import Muon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42717285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You need to generate a training function, it's also good practice to do so.\n",
    "def train_epoch(model, dataloader, optimizers, scheduler, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        for opt in optimizers:\n",
    "            opt.zero_grad()\n",
    "        outputs = model(inputs, attention_mask)\n",
    "        # Debugging output shapes\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        # Note the gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # in the training step\n",
    "        for opt in optimizers:\n",
    "            opt.step()\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        scheduler.step()\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e125a0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Muon in module muon:\n",
      "\n",
      "class Muon(torch.optim.optimizer.Optimizer)\n",
      " |  Muon(params, lr=0.02, weight_decay=0.01, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1)\n",
      " |  \n",
      " |  Muon - MomentUm Orthogonalized by Newton-schulz\n",
      " |  \n",
      " |  https://kellerjordan.github.io/posts/muon/\n",
      " |  \n",
      " |  Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
      " |  processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
      " |  matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
      " |  the advantage that it can be stably run in bfloat16 on the GPU.\n",
      " |  \n",
      " |  Some warnings:\n",
      " |  - This optimizer should not be used for the embedding layer, the final fully connected layer,\n",
      " |  or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).\n",
      " |  - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      lr: The learning rate used by the internal SGD.\n",
      " |      momentum: The momentum used by the internal SGD.\n",
      " |      nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n",
      " |      ns_steps: The number of Newton-Schulz iteration steps to use.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Muon\n",
      " |      torch.optim.optimizer.Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  step(self)\n",
      " |      Perform a single optimization step to update parameter.\n",
      " |      \n",
      " |      Args:\n",
      " |          closure (Callable): A closure that reevaluates the model and\n",
      " |              returns the loss. Optional for most optimizers.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __getstate__(self) -> Dict[str, Any]\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state: Dict[str, Any]) -> None\n",
      " |  \n",
      " |  add_param_group(self, param_group: Dict[str, Any]) -> None\n",
      " |      Add a param group to the :class:`Optimizer` s `param_groups`.\n",
      " |      \n",
      " |      This can be useful when fine tuning a pre-trained network as frozen layers can be made\n",
      " |      trainable and added to the :class:`Optimizer` as training progresses.\n",
      " |      \n",
      " |      Args:\n",
      " |          param_group (dict): Specifies what Tensors should be optimized along with group\n",
      " |              specific optimization options.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Dict[str, Any]) -> None\n",
      " |      Load the optimizer state.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): optimizer state. Should be an object returned\n",
      " |              from a call to :meth:`state_dict`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          The names of the parameters (if they exist under the \"param_names\" key of each param group\n",
      " |          in :meth:`state_dict`) will not affect the loading process.\n",
      " |          To use the parameters' names for custom cases (such as when the parameters in the loaded state dict\n",
      " |          differ from those initialized in the optimizer),\n",
      " |          a custom ``register_load_state_dict_pre_hook`` should be implemented to adapt the loaded dict\n",
      " |          accordingly.\n",
      " |          If ``param_names`` exist in loaded state dict ``param_groups`` they will be saved and override\n",
      " |          the current names, if present, in the optimizer state. If they do not exist in loaded state dict,\n",
      " |          the optimizer ``param_names`` will remain unchanged.\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a load_state_dict post-hook which will be called after\n",
      " |      :meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(optimizer) -> None\n",
      " |      \n",
      " |      The ``optimizer`` argument is the optimizer instance being used.\n",
      " |      \n",
      " |      The hook will be called with argument ``self`` after calling\n",
      " |      ``load_state_dict`` on ``self``. The registered hook can be used to\n",
      " |      perform post-processing after ``load_state_dict`` has loaded the\n",
      " |      ``state_dict``.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If True, the provided post ``hook`` will be fired before\n",
      " |              all the already registered post-hooks on ``load_state_dict``. Otherwise,\n",
      " |              the provided ``hook`` will be fired after all the already registered\n",
      " |              post-hooks. (default: False)\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemoveableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a load_state_dict pre-hook which will be called before\n",
      " |      :meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(optimizer, state_dict) -> state_dict or None\n",
      " |      \n",
      " |      The ``optimizer`` argument is the optimizer instance being used and the\n",
      " |      ``state_dict`` argument is a shallow copy of the ``state_dict`` the user\n",
      " |      passed in to ``load_state_dict``. The hook may modify the state_dict inplace\n",
      " |      or optionally return a new one. If a state_dict is returned, it will be used\n",
      " |      to be loaded into the optimizer.\n",
      " |      \n",
      " |      The hook will be called with argument ``self`` and ``state_dict`` before\n",
      " |      calling ``load_state_dict`` on ``self``. The registered hook can be used to\n",
      " |      perform pre-processing before the ``load_state_dict`` call is made.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If True, the provided pre ``hook`` will be fired before\n",
      " |              all the already registered pre-hooks on ``load_state_dict``. Otherwise,\n",
      " |              the provided ``hook`` will be fired after all the already registered\n",
      " |              pre-hooks. (default: False)\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemoveableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a state dict post-hook which will be called after :meth:`~torch.optim.Optimizer.state_dict` is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(optimizer, state_dict) -> state_dict or None\n",
      " |      \n",
      " |      The hook will be called with arguments ``self`` and ``state_dict`` after generating\n",
      " |      a ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\n",
      " |      return a new one. The registered hook can be used to perform post-processing\n",
      " |      on the ``state_dict`` before it is returned.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If True, the provided post ``hook`` will be fired before\n",
      " |              all the already registered post-hooks on ``state_dict``. Otherwise,\n",
      " |              the provided ``hook`` will be fired after all the already registered\n",
      " |              post-hooks. (default: False)\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemoveableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(optimizer) -> None\n",
      " |      \n",
      " |      The ``optimizer`` argument is the optimizer instance being used.\n",
      " |      The hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\n",
      " |      The registered hook can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If True, the provided pre ``hook`` will be fired before\n",
      " |              all the already registered pre-hooks on ``state_dict``. Otherwise,\n",
      " |              the provided ``hook`` will be fired after all the already registered\n",
      " |              pre-hooks. (default: False)\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemoveableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_step_post_hook(self, hook: Callable[[typing_extensions.Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register an optimizer step post hook which will be called after optimizer step.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(optimizer, args, kwargs) -> None\n",
      " |      \n",
      " |      The ``optimizer`` argument is the optimizer instance being used.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_step_pre_hook(self, hook: Callable[[typing_extensions.Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register an optimizer step pre hook which will be called before optimizer step.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(optimizer, args, kwargs) -> None or modified args and kwargs\n",
      " |      \n",
      " |      The ``optimizer`` argument is the optimizer instance being used. If\n",
      " |      args and kwargs are modified by the pre-hook, then the transformed\n",
      " |      values are returned as a tuple containing the new_args and new_kwargs.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  state_dict(self) -> Dict[str, Any]\n",
      " |      Return the state of the optimizer as a :class:`dict`.\n",
      " |      \n",
      " |      It contains two entries:\n",
      " |      \n",
      " |      * ``state``: a Dict holding current optimization state. Its content\n",
      " |          differs between optimizer classes, but some common characteristics\n",
      " |          hold. For example, state is saved per parameter, and the parameter\n",
      " |          itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n",
      " |          to a Dict with state corresponding to each parameter.\n",
      " |      * ``param_groups``: a List containing all parameter groups where each\n",
      " |          parameter group is a Dict. Each parameter group contains metadata\n",
      " |          specific to the optimizer, such as learning rate and weight decay,\n",
      " |          as well as a List of parameter IDs of the parameters in the group.\n",
      " |          If a param group was initialized with ``named_parameters()`` the names\n",
      " |          content will also be saved in the state dict.\n",
      " |      \n",
      " |      NOTE: The parameter IDs may look like indices but they are just IDs\n",
      " |      associating state with param_group. When loading from a state_dict,\n",
      " |      the optimizer will zip the param_group ``params`` (int IDs) and the\n",
      " |      optimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\n",
      " |      match state WITHOUT additional verification.\n",
      " |      \n",
      " |      A returned state dict might look something like:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          {\n",
      " |              'state': {\n",
      " |                  0: {'momentum_buffer': tensor(...), ...},\n",
      " |                  1: {'momentum_buffer': tensor(...), ...},\n",
      " |                  2: {'momentum_buffer': tensor(...), ...},\n",
      " |                  3: {'momentum_buffer': tensor(...), ...}\n",
      " |              },\n",
      " |              'param_groups': [\n",
      " |                  {\n",
      " |                      'lr': 0.01,\n",
      " |                      'weight_decay': 0,\n",
      " |                      ...\n",
      " |                      'params': [0]\n",
      " |                      'param_names' ['param0']  (optional)\n",
      " |                  },\n",
      " |                  {\n",
      " |                      'lr': 0.001,\n",
      " |                      'weight_decay': 0.5,\n",
      " |                      ...\n",
      " |                      'params': [1, 2, 3]\n",
      " |                      'param_names': ['param1', 'layer.weight', 'layer.bias'] (optional)\n",
      " |                  }\n",
      " |              ]\n",
      " |          }\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Reset the gradients of all optimized :class:`torch.Tensor` s.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              This will in general have lower memory footprint, and can modestly improve performance.\n",
      " |              However, it changes certain behaviors. For example:\n",
      " |              1. When the user tries to access a gradient and perform manual ops on it,\n",
      " |              a None attribute or a Tensor full of 0s will behave differently.\n",
      " |              2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s\n",
      " |              are guaranteed to be None for params that did not receive a gradient.\n",
      " |              3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n",
      " |              (in one case it does the step with a gradient of 0 and in the other it skips\n",
      " |              the step altogether).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  profile_hook_step(func: Callable[[~_P], ~R]) -> Callable[[~_P], ~R]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  OptimizerPostHook = typing.Callable[[typing_extensions.Self, typing......\n",
      " |  \n",
      " |  OptimizerPreHook = typing.Callable[[typing_extensions.Self, typing.......\n",
      " |  \n",
      " |  __annotations__ = {'OptimizerPostHook': typing_extensions.TypeAlias, '...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Muon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5ea683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "\n",
    "# Initialize the process group with the 'nccl' backend and TCP-based discovery\n",
    "dist.init_process_group(backend='nccl', init_method='tcp://127.0.0.1:51000', world_size=1, rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0fd51042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-11 14:43:26,835] A new study created in memory with name: no-name-7b4bc429-afa2-4506-8f98-c2d7a10a5350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.23138832997987926\n",
      "1\n",
      "0.23138832997987926\n",
      "2\n",
      "0.23138832997987926\n",
      "3\n",
      "0.23138832997987926\n",
      "4\n",
      "0.23138832997987926\n",
      "5\n",
      "0.6861167002012073\n",
      "6\n",
      "0.23138832997987926\n",
      "7\n",
      "0.23138832997987926\n",
      "8\n",
      "0.7535211267605634\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-11 14:46:32,645] Trial 0 finished with value: 0.6862265133042211 and parameters: {'embed_dim': 256, 'nhead': 2, 'num_layers': 3, 'dropout': 0.3996054369150499, 'lr': 0.00043456229507846773, 'batch_size': 512}. Best is trial 0 with value: 0.6862265133042211.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7535211267605634\n",
      "0\n",
      "0.23138832997987926\n",
      "1\n",
      "0.23138832997987926\n",
      "2\n",
      "0.7454728370221329\n",
      "3\n",
      "0.6740442655935613\n",
      "4\n",
      "0.6629778672032193\n",
      "5\n",
      "0.7313883299798792\n",
      "6\n",
      "0.7323943661971831\n",
      "7\n",
      "0.7152917505030181\n",
      "8\n",
      "0.7022132796780685\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-11 14:49:07,465] Trial 1 finished with value: 0.5758691293372954 and parameters: {'embed_dim': 64, 'nhead': 4, 'num_layers': 3, 'dropout': 0.18354112854748, 'lr': 0.006730967320945705, 'batch_size': 512}. Best is trial 1 with value: 0.5758691293372954.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7152917505030181\n",
      "0\n",
      "0.7686116700201208\n",
      "1\n",
      "0.7344064386317908\n",
      "2\n",
      "0.704225352112676\n",
      "3\n",
      "0.6096579476861167\n",
      "4\n",
      "0.6971830985915493\n",
      "5\n",
      "0.7132796780684104\n",
      "6\n",
      "0.6247484909456741\n",
      "7\n",
      "0.6579476861167002\n",
      "8\n",
      "0.630784708249497\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-11 14:51:40,166] Trial 2 finished with value: 0.6005610419471019 and parameters: {'embed_dim': 64, 'nhead': 1, 'num_layers': 3, 'dropout': 0.43695408096549937, 'lr': 0.004604097537256577, 'batch_size': 128}. Best is trial 1 with value: 0.5758691293372954.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.670020120724346\n",
      "0\n",
      "0.23138832997987926\n",
      "1\n",
      "0.23138832997987926\n",
      "2\n",
      "0.5583501006036218\n",
      "3\n",
      "0.7505030181086519\n",
      "4\n",
      "0.7364185110663984\n",
      "5\n",
      "0.6368209255533199\n",
      "6\n",
      "0.7344064386317908\n",
      "7\n",
      "0.7354124748490946\n",
      "8\n",
      "0.676056338028169\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-11 14:54:57,349] Trial 3 finished with value: 0.5706504096447582 and parameters: {'embed_dim': 256, 'nhead': 2, 'num_layers': 3, 'dropout': 0.29331254792873557, 'lr': 0.004957341273659826, 'batch_size': 512}. Best is trial 3 with value: 0.5706504096447582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6770623742454729\n",
      "Best trial:\n",
      "  Validation Loss: 0.5706504096447582\n",
      "  Best hyperparameters: {'embed_dim': 256, 'nhead': 2, 'num_layers': 3, 'dropout': 0.29331254792873557, 'lr': 0.004957341273659826, 'batch_size': 512}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# For optuna, it's good to generate a function that runs all your suggestions through.\n",
    "def objective(trial):\n",
    "    #This is what we'll test, but it could be anything, HEHE!\n",
    "    embed_dim = trial.suggest_categorical(\"embed_dim\", [64, 128, 256])\n",
    "    nhead = trial.suggest_categorical(\"nhead\", [1, 2, 4])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [128,256,512])\n",
    "    # Make the transformer\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    model = SmallFlashTransformer(vocab_size, embed_dim, nhead, num_layers, dropout).to(device)\n",
    "    #NOTE this: Weighted classes for CrossEntropy loss\n",
    "    class_counts = df['LABEL'].value_counts().to_list()\n",
    "    weights = [1.0 / count for count in class_counts]\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    #Note the weight and label smoothing\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "    #Note the difference between Adam and Adam Weight Decay\n",
    "    # Find ≥2D parameters in the body of the network -- these should be optimized by Muon\n",
    "    muon_params = [p for p in model.layers.parameters() if p.ndim >= 2]\n",
    "    # Find everything else -- these should be optimized by AdamW\n",
    "    adamw_params = ([p for p in model.layers.parameters() if p.ndim < 2]\n",
    "                  + [*model.fc1.parameters(),*model.fc2.parameters(), *model.embedding.parameters()])\n",
    "    # Create the optimizer\n",
    "    optimizers = [Muon(muon_params, lr=0.02, momentum=0.95),\n",
    "                  torch.optim.AdamW(adamw_params, lr=3e-4, betas=(0.90, 0.95), weight_decay=0.01)]\n",
    "\n",
    "    #Note difference between StepLR and OneCycleLR\n",
    "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1) \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizers[1],\n",
    "            max_lr=lr,\n",
    "            epochs=10,\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            pct_start=0.1\n",
    "        )\n",
    "    # Generate data loaders\n",
    "    # Set how many epocs you want\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)\n",
    "        train_loss = train_epoch(model, train_loader, optimizers, scheduler,criterion)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "        print(val_acc)\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_loss\n",
    "# Do you want to minimize or maximize the objective (val_loss?)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "# How many trials? optuna has several optimization algoirthms included\n",
    "study.optimize(objective, n_trials=20, timeout=600)\n",
    "#https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html Oh god, there's so many of them.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Validation Loss:\", trial.value)\n",
    "print(\"  Best hyperparameters:\", trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfacf5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
