{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a060a54-820d-455e-a9e2-f855c54f14f5",
   "metadata": {},
   "source": [
    "# Lab 2: Deep Networks\n",
    "## Goal of this lab:\n",
    "1. How to build custom datasets and perpare data for training with dataloaders\n",
    "2. How to build and train a neural network\n",
    "3. Automatic differentiation with `autograd`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf5bd4-5722-4c77-9474-20beada7399e",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders\n",
    "PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the Dataset to enable easy access to the samples. Today, we focus on creating a custom dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26512c58-f87f-40e9-9a68-2079e7b61e38",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "In a custom Dataset class, we must implement three functions: `__init__`, `__len__`, and `__getitem__`. Here is a sample dataset class:\n",
    "```python\n",
    "class RegressionDataset(torch.utils.data.Dataset):    \n",
    "    def __init__(self, x, y):\n",
    "        super().__init__() \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "```\n",
    "\n",
    "#### 1. `__init__`\n",
    "The `__init__` function is run once when instantiating the Dataset object. Here, we pass the input (`x`) and the label (`y`) and initialize them. \n",
    "\n",
    "#### 2. `__len__`\n",
    "The `__len__` function returns the number of samples in our dataset.\n",
    "\n",
    "#### 3. `__getitem__`\n",
    "The `__getitem__` function loads and returns a sample from the dataset at the given index `idx`. Here, we return the `x` and `y` at index `idx`. \n",
    "\n",
    "Now, let's create a dummy dataset and implement a `Dataset` for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed817238-5e6d-4f5c-8f46-876185cca721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# here, we use scikit-learn's make_regression function to create a dummy datasets with 32 features as input and a number as target\n",
    "data_x, data_y = make_regression(n_samples = 1000, n_features = 32, random_state=0)\n",
    "data_x = data_x.astype(np.float32)\n",
    "data_y = data_y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f76b3b-7c70-4ff1-8d45-00b2b0c8b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the dataset here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362b228e-de3c-4901-a189-15d97613f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RegressionDataset(data_x, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d0afe-d7cb-4147-ac1d-57e4543cbae8",
   "metadata": {},
   "source": [
    "### Dataloaders\n",
    "The `Dataset` retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s `multiprocessing` to speed up data retrieval.\n",
    "\n",
    "`DataLoader` is an iterable that abstracts this complexity for us in an easy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a53ae442-3b33-416d-bc41-74948723ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# create a dataloader with a batch size of 64, and set shuffling to true. \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c187c914-59e7-4ca5-9d39-1983492d65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to see if it works\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69847cc3-62d7-40a6-9f69-e777003af39e",
   "metadata": {},
   "source": [
    "## Build the neural network\n",
    "\n",
    "The `torch.nn` namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the `nn.Module`. A neural network is a module itself that consists of other modules (layers). \n",
    "\n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7776fa20-8ec5-4384-a0d2-9e8a2adf68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Linear == Dense == Fully Connected\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508de031-ab8c-4b24-86ed-089fde564421",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a975d5-3488-420c-983b-bcee85492c13",
   "metadata": {},
   "source": [
    "### Train the neural network\n",
    "\n",
    "Now that we have our dataset and model setup, it is time to train out model by optimizing the parameters on our data. Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters, and optimizes these parameters using gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b8a0f-cdc1-4751-acdc-31a47decd00d",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "Hyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates (read more about hyperparameter tuning)\n",
    "\n",
    "We define the following hyperparameters for training:\n",
    "* **Number of Epochs:** the number times to iterate over the dataset\n",
    "* **Batch Size:** the number of data samples propagated through the network before the parameters are updated\n",
    "* **Learning Rate:** how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe9ca2f8-a5d1-4f0b-bba2-70d839b8e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameters:\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b20454d6-bc56-478e-8c83-767f93fd1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b6090-056c-466f-a97c-418e5e4548bf",
   "metadata": {},
   "source": [
    "#### Loss function (Criterion)\n",
    "When presented with some training data, our untrained network is likely not to give the correct answer. Loss function measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value. \n",
    "\n",
    "Since we're doing a regression task, we will use mean squared error as our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17421ed9-b871-4132-ac92-14f7e6d8eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a8b39-9280-4160-bd56-e251a8c26f37",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "\n",
    "Optimization is the process of adjusting model parameters to reduce model error in each training step. Here we use Stochastic Gradient Descent (SGD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86afef30-18b7-462a-bee1-98e04bf0cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb10b3-f027-4782-a39a-bc36c36eb5dd",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "Inside the training loop, once the model prediction is made, optimization happens in three steps:\n",
    "* Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "* Backpropagate the prediction loss with a call to `criterion.backward()`. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "* Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass.\n",
    "\n",
    "Let's implement the loop below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21910f-f488-4c02-9681-6b566d6e0bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "\n",
    "training_loss = []\n",
    "for epoch in trange(EPOCHS): # for each EPOCH\n",
    "    for (x,y) in train_dataloader: # for each BATCH\n",
    "        # make prediction\n",
    "        \n",
    "        # calculate loss\n",
    "        \n",
    "        # reset gradient\n",
    "        # backpropagate\n",
    "        # update the weights\n",
    "        \n",
    "        # save loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdac0350-a109-4261-84a2-e84ab7994b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the loss function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(loss):\n",
    "    x = np.arange(len(loss))\n",
    "    plt.figure(dpi=150)\n",
    "    plt.plot(x, loss)\n",
    "    plt.xlabel('Step/Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36a5a6-d7a2-4b36-b517-5bec397a7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4acbc67-ed6d-4011-99d1-ebfa2a8bb1a0",
   "metadata": {},
   "source": [
    "### Automatic Differentiation with `torch.autograd`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28fc630-05e1-403e-b545-15d24cd078b8",
   "metadata": {},
   "source": [
    "When training neural networks, the most frequently used algorithm is **back propagation**. In this algorithm, parameters (model weights) are adjusted according to the **gradient** of the loss function with respect to the given parameter.\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called `torch.autograd`. It supports automatic computation of gradient for any computational graph.\n",
    "\n",
    "Consider the simplest one-layer neural network, with input `x`, parameters `w` and `b`, and some loss function. It can be defined in PyTorch in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b163933a-0461-4cb0-8e9e-658285c9ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc580bb",
   "metadata": {},
   "source": [
    "In this network, `w` and `b` are parameters, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables. In order to do that, we set the `requires_grad` property of those tensors. Note: this property can also be set later using `x.requires_grad_(True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05f0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49465897",
   "metadata": {},
   "source": [
    "#### Compute the graident\n",
    "\n",
    "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely, we need $\\frac{d loss}{dw}$​ and $\\frac{d loss}{db}$ under some fixed values of $x$ and $y$. To compute those derivatives, we call `loss.backward()`, and then retrieve the values from `w.grad` and `b.grad`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82fc6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gradient\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b993984",
   "metadata": {},
   "source": [
    "#### Disabling Gradient Tracking\n",
    "By default, all tensors with `requires_grad=True` are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with `torch.no_grad()` block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ec831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable gradient tracking\n",
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183dd17-c123-4e59-b10e-8c53fbef5311",
   "metadata": {},
   "source": [
    "#### Implement the above neural network with `torch.autograd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af19b519-7ea2-48e8-bb95-66a0e4610985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# recreate the linear layer above\n",
    "w = \n",
    "b = \n",
    "\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff9532-9071-43fb-a2fe-89094a937a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_manual = []\n",
    "for epoch in trange(EPOCHS): # for each EPOCH\n",
    "    for (x,y) in train_dataloader: # for each BATCH\n",
    "        # make prediction\n",
    "        \n",
    "        # calculate loss\n",
    "\n",
    "        # reset gradient\n",
    "\n",
    "        \n",
    "        # backpropagate\n",
    "\n",
    "        \n",
    "        # update the weights\n",
    "            \n",
    "        # save loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0939b45-d3b1-45bd-8d96-0da057a32943",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(training_loss_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643f0d4-8aba-41de-ba48-d4e9ea5d820f",
   "metadata": {},
   "source": [
    "#### Reference: \n",
    "Datasets and Dataloaders: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html  \n",
    "Build the Neural Network: https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html  \n",
    "Automatic Differentiation with `torch.autograd`: https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
