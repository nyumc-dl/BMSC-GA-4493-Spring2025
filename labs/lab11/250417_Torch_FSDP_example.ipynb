{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fcdd39-63d2-44ae-be08-07c256a91d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://arxiv.org/abs/2304.11277\n",
    "#https://pytorch.org/tutorials/intermediate/FSDP_advanced_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec659aab-50a1-4aec-8a75-0deec7ca38a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crazy ahh imports\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, GPT2TokenizerFast\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration #Model\n",
    "import functools\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP #Important\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers.models.t5.modeling_t5 import T5Block\n",
    "\n",
    "#Checkpointing\n",
    "from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n",
    " checkpoint_wrapper,\n",
    " CheckpointImpl,\n",
    " apply_activation_checkpointing)\n",
    "\n",
    "#All the FSDP part\n",
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP,\n",
    "    MixedPrecision,\n",
    "    BackwardPrefetch,\n",
    "    ShardingStrategy,\n",
    "    FullStateDictConfig,\n",
    "    StateDictType,\n",
    ")\n",
    "from torch.distributed.fsdp.wrap import (\n",
    "    transformer_auto_wrap_policy,\n",
    "    enable_wrap,\n",
    "    wrap,\n",
    ")\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers.models.t5.modeling_t5 import T5Block\n",
    "from typing import Type\n",
    "import time\n",
    "import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb40c1c8-3cd2-4014-94c8-1c161e31bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from nlp import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "class wikihow(Dataset):\n",
    "    def __init__(self, tokenizer, type_path, num_samples, input_length, output_length, print_text=False):\n",
    "        self.dataset =  load_dataset('wikihow', 'all', data_dir='data/', split=type_path)\n",
    "        if num_samples:\n",
    "            self.dataset = self.dataset.select(list(range(0, num_samples)))\n",
    "        self.input_length = input_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.output_length = output_length\n",
    "        self.print_text = print_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.replace('Example of text:', '')\n",
    "        text = text.replace('Example of Summary:', '')\n",
    "        text = text.replace('\\n','')\n",
    "        text = text.replace('``', '')\n",
    "        text = text.replace('\"', '')\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    def convert_to_features(self, example_batch):\n",
    "        # Tokenize contexts and questions (as pairs of inputs)\n",
    "\n",
    "        if self.print_text:\n",
    "            print(\"Input Text: \", self.clean_text(example_batch['text']))\n",
    "#         input_ = self.clean_text(example_batch['text']) + \" </s>\"\n",
    "#         target_ = self.clean_text(example_batch['headline']) + \" </s>\"\n",
    "\n",
    "        input_ = self.clean_text(example_batch['text'])\n",
    "        target_ = self.clean_text(example_batch['headline'])\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([input_], max_length=self.input_length,\n",
    "                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        targets = self.tokenizer.batch_encode_plus([target_], max_length=self.output_length,\n",
    "                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "        return source, targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source, targets = self.convert_to_features(self.dataset[index])\n",
    "\n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask    = source[\"attention_mask\"].squeeze()\n",
    "        target_mask = targets[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "\n",
    "def get_dataset(tokenizer, type_path, num_samples, args):\n",
    "      return wikihow(tokenizer=tokenizer, type_path=type_path, num_samples=num_samples,  input_length=max_input_length,\n",
    "                        output_length=max_output_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377ec55b-710d-4949-b01c-aa9e6d1b8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"nccl\")\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42afaa18-437e-42ed-9469-122911ad10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(model_name):\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    tokenizer =  T5Tokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b61b327-9410-4f1f-9a52-0fbbdb7d3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_of_run():\n",
    "    \"\"\"create date and time for file save uniqueness\n",
    "    example: 2022-05-07-08:31:12_PM'\n",
    "    \"\"\"\n",
    "    date_of_run = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n",
    "    print(f\"--> current date and time of run = {date_of_run}\")\n",
    "    return date_of_run\n",
    "\n",
    "def format_metrics_to_gb(item):\n",
    "    \"\"\"quick function to format numbers to gigabyte and round to 4 digit precision\"\"\"\n",
    "    metric_num = item / g_gigabyte\n",
    "    metric_num = round(metric_num, ndigits=4)\n",
    "    return metric_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9760c884-0611-47c5-b9a0-84ffe142151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n",
    "    model.train()\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    fsdp_loss = torch.zeros(2).to(local_rank)\n",
    "\n",
    "    if sampler:\n",
    "        sampler.set_epoch(epoch)\n",
    "    if rank==0:\n",
    "        inner_pbar = tqdm.tqdm(\n",
    "            range(len(train_loader)), colour=\"blue\", desc=\"r0 Training Epoch\"\n",
    "        )\n",
    "    for batch in train_loader:\n",
    "        for key in batch.keys():\n",
    "            batch[key] = batch[key].to(local_rank)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"] )\n",
    "        loss = output[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        fsdp_loss[0] += loss.item()\n",
    "        fsdp_loss[1] += len(batch)\n",
    "        if rank==0:\n",
    "            inner_pbar.update(1)\n",
    "\n",
    "    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)\n",
    "    train_accuracy = fsdp_loss[0] / fsdp_loss[1]\n",
    "\n",
    "\n",
    "    if rank == 0:\n",
    "        inner_pbar.close()\n",
    "        print(\n",
    "                f\"Train Epoch: \\t{epoch}, Loss: \\t{train_accuracy:.4f}\"\n",
    "            )\n",
    "    return train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51fac0e6-a550-4e16-b681-8e38aec80582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, rank, world_size, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    fsdp_loss = torch.zeros(3).to(local_rank)\n",
    "    if rank == 0:\n",
    "        inner_pbar = tqdm.tqdm(\n",
    "            range(len(val_loader)), colour=\"green\", desc=\"Validation Epoch\"\n",
    "        )\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            for key in batch.keys():\n",
    "                batch[key] = batch[key].to(local_rank)\n",
    "            output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"])\n",
    "            fsdp_loss[0] += output[\"loss\"].item()  # sum up batch loss\n",
    "            fsdp_loss[1] += len(batch)\n",
    "\n",
    "            if rank==0:\n",
    "                inner_pbar.update(1)\n",
    "\n",
    "    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)\n",
    "    val_loss = fsdp_loss[0] / fsdp_loss[1]\n",
    "    if rank == 0:\n",
    "        inner_pbar.close()\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51684448-7d98-48c1-9d2f-ae743c8c8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fsdp_main(args):\n",
    "\n",
    "    model, tokenizer = setup_model(\"t5-base\")\n",
    "\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    rank = int(os.environ['RANK'])\n",
    "    world_size = int(os.environ['WORLD_SIZE'])\n",
    "\n",
    "\n",
    "    dataset = load_dataset('wikihow', 'all', data_dir='data/')\n",
    "    print(dataset.keys())\n",
    "    print(\"Size of train dataset: \", dataset['train'].shape)\n",
    "    print(\"Size of Validation dataset: \", dataset['validation'].shape)\n",
    "\n",
    "\n",
    "    #wikihow(tokenizer, type_path, num_samples, input_length, output_length, print_text=False)\n",
    "    train_dataset = wikihow(tokenizer, 'train', 1500, 512, 150, False)\n",
    "    val_dataset = wikihow(tokenizer, 'validation', 300, 512, 150, False)\n",
    "\n",
    "    sampler1 = DistributedSampler(train_dataset, rank=rank, num_replicas=world_size, shuffle=True)\n",
    "    sampler2 = DistributedSampler(val_dataset, rank=rank, num_replicas=world_size)\n",
    "\n",
    "    setup()\n",
    "\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}\n",
    "    cuda_kwargs = {'num_workers': 2,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': False}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, **test_kwargs)\n",
    "\n",
    "    t5_auto_wrap_policy = functools.partial(\n",
    "        transformer_auto_wrap_policy,\n",
    "        transformer_layer_cls={\n",
    "            T5Block,\n",
    "        },\n",
    "    )\n",
    "    sharding_strategy: ShardingStrategy = ShardingStrategy.SHARD_GRAD_OP #for Zero2 and FULL_SHARD for Zero3\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "\n",
    "    #init_start_event = torch.cuda.Event(enable_timing=True)\n",
    "    #init_end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    #init_start_event.record()\n",
    "\n",
    "    bf16_ready = (\n",
    "    torch.version.cuda\n",
    "    and torch.cuda.is_bf16_supported()\n",
    "    and LooseVersion(torch.version.cuda) >= \"11.0\"\n",
    "    and dist.is_nccl_available()\n",
    "    and nccl.version() >= (2, 10)\n",
    "    )\n",
    "\n",
    "    if bf16_ready:\n",
    "        mp_policy = bfSixteen\n",
    "    else:\n",
    "        mp_policy = None # defaults to fp32\n",
    "\n",
    "    # model is on CPU before input to FSDP\n",
    "    model = FSDP(model,\n",
    "        auto_wrap_policy=t5_auto_wrap_policy,\n",
    "        mixed_precision=mp_policy,\n",
    "        #sharding_strategy=sharding_strategy,\n",
    "        device_id=torch.cuda.current_device())\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    curr_val_loss = float(\"inf\")\n",
    "    file_save_name = \"T5-model-\"\n",
    "\n",
    "    if rank == 0:\n",
    "        time_of_run = get_date_of_run()\n",
    "        dur = []\n",
    "        train_acc_tracking = []\n",
    "        val_acc_tracking = []\n",
    "        training_start_time = time.time()\n",
    "\n",
    "    if rank == 0 and args.track_memory:\n",
    "        mem_alloc_tracker = []\n",
    "        mem_reserved_tracker = []\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        t0 = time.time()\n",
    "        train_accuracy = train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n",
    "        if args.run_validation:\n",
    "            curr_val_loss = validation(model, rank, world_size, val_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        if rank == 0:\n",
    "\n",
    "            print(f\"--> epoch {epoch} completed...entering save and stats zone\")\n",
    "\n",
    "            dur.append(time.time() - t0)\n",
    "            train_acc_tracking.append(train_accuracy.item())\n",
    "\n",
    "            if args.run_validation:\n",
    "                val_acc_tracking.append(curr_val_loss.item())\n",
    "\n",
    "            if args.track_memory:\n",
    "                mem_alloc_tracker.append(\n",
    "                    format_metrics_to_gb(torch.cuda.memory_allocated())\n",
    "                )\n",
    "                mem_reserved_tracker.append(\n",
    "                    format_metrics_to_gb(torch.cuda.memory_reserved())\n",
    "                )\n",
    "            print(f\"completed save and stats zone...\")\n",
    "\n",
    "        if args.save_model and curr_val_loss < best_val_loss:\n",
    "\n",
    "            # save\n",
    "            if rank == 0:\n",
    "                print(f\"--> entering save model state\")\n",
    "\n",
    "            save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n",
    "            with FSDP.state_dict_type(\n",
    "                model, StateDictType.FULL_STATE_DICT, save_policy\n",
    "            ):\n",
    "                cpu_state = model.state_dict()\n",
    "            #print(f\"saving process: rank {rank}  done w state_dict\")\n",
    "\n",
    "\n",
    "            if rank == 0:\n",
    "                print(f\"--> saving model ...\")\n",
    "                currEpoch = (\n",
    "                    \"-\" + str(epoch) + \"-\" + str(round(curr_val_loss.item(), 4)) + \".pt\"\n",
    "                )\n",
    "                print(f\"--> attempting to save model prefix {currEpoch}\")\n",
    "                save_name = file_save_name + \"-\" + time_of_run + \"-\" + currEpoch\n",
    "                print(f\"--> saving as model name {save_name}\")\n",
    "\n",
    "                torch.save(cpu_state, save_name)\n",
    "\n",
    "        if curr_val_loss < best_val_loss:\n",
    "\n",
    "            best_val_loss = curr_val_loss\n",
    "            if rank==0:\n",
    "                print(f\"-->>>> New Val Loss Record: {best_val_loss}\")\n",
    "\n",
    "    dist.barrier()\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfca5ddc-0272-4dec-9d4e-89d828b8af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch T5 FSDP Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=4, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=4, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=2, metavar='N',\n",
    "                        help='number of epochs to train (default: 3)')\n",
    "    parser.add_argument('--lr', type=float, default=.002, metavar='LR',\n",
    "                        help='learning rate (default: .002)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--track_memory', action='store_false', default=True,\n",
    "                        help='track the gpu memory')\n",
    "    parser.add_argument('--run_validation', action='store_false', default=True,\n",
    "                        help='running the validation')\n",
    "    parser.add_argument('--save-model', action='store_false', default=True,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    fsdp_main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287fec59-4917-4bdb-bb2c-462d1fd411fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
