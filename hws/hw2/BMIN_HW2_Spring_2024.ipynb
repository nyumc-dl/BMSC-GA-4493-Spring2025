{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQxe87HrvH_-"
   },
   "source": [
    "# Homework 2 - Convolutional Neural Networks\n",
    "\n",
    "### Deep Learning in Medicine - Spring 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykTROO96vH__"
   },
   "source": [
    "\n",
    "\n",
    "**Note:** If you need to write mathematical terms, you can type your answeres in a Markdown Cell via LaTex\n",
    "\n",
    "**See:** <a href=\"https://stackoverflow.com/questions/13208286/how-to-write-latex-in-ipython-notebook\">here</a> if you have issues. To see basic LaTex notation see: <a href=\"https://en.wikibooks.org/wiki/LaTeX/Mathematics\"> here </a>.\n",
    "\n",
    "**Submission instruction:** Upload and Submit a zipped folder named netid_hw2 consisting of your final jupyter notebook and necessary files in <a href='https://brightspace.nyu.edu/d2l/home/427921'>Brightspace</a>. If you use code or script from web, please give a link to the code in your answers. Not providing the reference of the code used will reduce your points!!\n",
    "\n",
    "**Submission deadline: Saturday March 20rd, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6g2Z683hkjJ"
   },
   "source": [
    "### Topics & weightage -\n",
    "\n",
    "\n",
    "1.   Convolutions (30)\n",
    "2.   Network design (15)\n",
    "3.   Literature review (19)\n",
    "4.   Deep CNN design for disease classification (36)\n",
    "5.   Analysis of Results (5)\n",
    "6.   Bonus Questions (12) - optional!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39mt3bkz5X5c"
   },
   "source": [
    "## Question 1 Convolutions (Total 30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCmxfd4ccx8d"
   },
   "source": [
    "### 1.1 Convolutions from **scratch** for image processing (11 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5dPhf70ia2vU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "q3Du9n-7a22i"
   },
   "outputs": [],
   "source": [
    "# functions to plot images\n",
    "def plot_image(img: np.array):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img, cmap='gray');\n",
    "    \n",
    "def plot_two_images(img1: np.array, img2: np.array):\n",
    "    _, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].imshow(img1, cmap='gray')\n",
    "    ax[1].imshow(img2, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edjb74Kbqm4I"
   },
   "source": [
    "#### 1.1.a (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZQ4tQ2t4a28o"
   },
   "outputs": [],
   "source": [
    "# TODO: load any image of your choice and display (plot) the resized image (224*224) in grayscale using the plot_image function\n",
    "# or you can also utilize the sample image provided --> cat.png\n",
    "# (none of these transformations are mandatory, but they make our job a bit easier, \n",
    "# as there’s only one color channel to apply convolution to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cD3daIdwa2_6"
   },
   "outputs": [],
   "source": [
    "# defining filters \n",
    "sharpen = np.array([\n",
    "    [0, -1, 0],\n",
    "    [-1, 5, -1],\n",
    "    [0, -1, 0]\n",
    "])\n",
    "\n",
    "blur = np.array([\n",
    "    [0.0625, 0.125, 0.0625],\n",
    "    [0.125,  0.25,  0.125],\n",
    "    [0.0625, 0.125, 0.0625]\n",
    "])\n",
    "\n",
    "outline = np.array([\n",
    "    [-1, -1, -1],\n",
    "    [-1,  8, -1],\n",
    "    [-1, -1, -1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zusfAumqtCy"
   },
   "source": [
    "#### 1.1.b (1.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "llq1QbCKa3DV"
   },
   "outputs": [],
   "source": [
    "def calculate_target_size(img_size: int, kernel_size: int) -> tuple:\n",
    "  '''\n",
    "  Helper function to calculate the image size after applying the convolution\n",
    "  Basically calculates how many windows of the filter size you can fit to an image (assuming square image)\n",
    "  Applying a convolution to an image will make it smaller (assuming no padding). \n",
    "  The filter size determined how smaller the image will be after convolving.\n",
    "\n",
    "  Args:\n",
    "    img_size: size of one dimension of the input image (assuming its a square image)\n",
    "    kernel_size: size of one dimension of the kernel (a square kernel)\n",
    "\n",
    "  Returns:\n",
    "    size: dimensions of the output image\n",
    "\n",
    "  '''\n",
    "  # TODO: write a generic function that inputs an image size & kernel size to calculate the final size of the output\n",
    "  return size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRE23wt6qwDF"
   },
   "source": [
    "#### 1.1.c (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0QKq5HGwcFhC"
   },
   "outputs": [],
   "source": [
    "def convolve(img: np.array, kernel: np.array) -> np.array:\n",
    "  '''\n",
    "  The convolve() function calculates the target size and creates \n",
    "  a matrix of zeros with that shape, iterates over all rows and \n",
    "  columns of the image matrix, subsets it, and applies the convolution.\n",
    "\n",
    "  Args:\n",
    "    img: the input image as a numpy array\n",
    "    kernel: kernel as a numpy array\n",
    "\n",
    "  Returns:\n",
    "  convolved_img: output image after sliding the kernel over the input image  \n",
    "  '''\n",
    "  # TODO: implement the convolve function \n",
    "  # iterate over all rows and columns of the input image matrix\n",
    "  # subset the image based on the kernel size at each position and apply the convolution operation\n",
    "  return convolved_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeoVU_WKqyXZ"
   },
   "source": [
    "#### 1.1.d (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8-G9VaQocFkg"
   },
   "outputs": [],
   "source": [
    "# TODO: use the convolved function & the sharpen filter to obtain a sharpened image of your original input \n",
    "# TODO: print the sharpened image array named img_sharpened\n",
    "# TODO: use the plot_two_images function to plot the original image and sharpened image side by side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Plp8gMwAq0qn"
   },
   "source": [
    "#### 1.1.e (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D-3olrz6cFrJ"
   },
   "outputs": [],
   "source": [
    "def negative_to_zero(img: np.array) -> np.array:\n",
    "  '''\n",
    "  Args:\n",
    "    img: numpy array of image\n",
    "  \n",
    "  Returns:\n",
    "    img: all values less than zero are assigned zero in original image\n",
    "  '''\n",
    "  # TODO: the sharpened image is a little dull, thats because some values in the sharpened image \n",
    "  # are less than zero\n",
    "  # write a function that uses 0 as a threshold and converts all pixel values less than zero to zero\n",
    "  return img\n",
    "  \n",
    "# TODO: use the plot_two_images function to plot the original image and negative_to_zero sharpened image side by side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.f (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FmfVVjIbcFuc"
   },
   "outputs": [],
   "source": [
    "# TODO: use the convolved function & the blur filter to obtain a blurred image of your original input \n",
    "# TODO: print the blurred image array named img_blurred\n",
    "# TODO: use the plot_two_images function to plot the original image and blurred image side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Gln5RqK9chx0"
   },
   "outputs": [],
   "source": [
    "# TODO: use the convolved function & the outline filter to obtain a outlined image of your original input \n",
    "# TODO: print the outlined image array named img_outlined\n",
    "# TODO: use the plot_two_images function to plot the outlined image and original image side by side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-e75ApKc4bF"
   },
   "source": [
    "**Reminder:** Padding is essentially a “black” border around the image. It’s black because the values are zeros, and zeros represent the color black. The black borders don’t have any side effects on the calculations, as it’s just a multiplication with zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQWn6aNVsfzU"
   },
   "source": [
    "#### 1.1.g (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_9GksRCIdrmB"
   },
   "outputs": [],
   "source": [
    "def get_padding_width_per_side(kernel_size: int) -> int:\n",
    "    '''\n",
    "    Function that returns the number of pixels we need to \n",
    "    pad the image with on a single side, depending on the kernel size\n",
    "\n",
    "    Args:\n",
    "    kernel_size: filter size \n",
    "\n",
    "    Returns:\n",
    "    padding_width \n",
    "    '''\n",
    "    # TODO: simple integer division by 2\n",
    "    return padding_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "K49KFlPLdiGG"
   },
   "outputs": [],
   "source": [
    "pad_3x3 = get_padding_width_per_side(3)\n",
    "pad_5x5 = get_padding_width_per_side(5)\n",
    "print(\"padding for kernel size 3 is\", pad_3x3, \"and padding for kernel size 5 is\", pad_5x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F__L5hv5sjA5"
   },
   "source": [
    "#### 1.1.h (1.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "d46gqmL8ch3x"
   },
   "outputs": [],
   "source": [
    "def add_padding_to_image(img: np.array, padding_width: int) -> np.array:\n",
    "    '''\n",
    "    Function that adds padding to the image. \n",
    "    First, the function declares a matrix of zeros with a shape of image.shape + padding * 2. \n",
    "    The function then indexes the matrix so the padding is ignored and changes the zeros with the actual image values.\n",
    "\n",
    "    Args:\n",
    "      img: Original image numpy array\n",
    "      padding_width: obtained in the get padding function earlier\n",
    "\n",
    "    Returns:\n",
    "      img_with_padding: padded image\n",
    "    '''\n",
    "    # TODO: take your image and a padding width as input and return the image with the padding added\n",
    "    return img_with_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QM2TSdDuop8h"
   },
   "source": [
    "#### 1.1.i (1 point)\n",
    "\n",
    "In the above function add_padding_to_image, explore the possible reason for the multiplication of padding_width by 2 in step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4acxGFnMo-Q7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtt3zTnztHZc"
   },
   "source": [
    "#### 1.1.j (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3Jo8rMrxch65"
   },
   "outputs": [],
   "source": [
    "# TODO: use the add_padding_to_image function to obtain the padded image (kernel size of 3)\n",
    "img_with_padding_3x3 =\n",
    "\n",
    "print(img_with_padding_3x3.shape)\n",
    "plot_image(img_with_padding_3x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "N_3ytccYciBC"
   },
   "outputs": [],
   "source": [
    "# TODO: use the add_padding_to_image function to obtain the padded image (kernel size of 5)\n",
    "img_with_padding_5x5 = \n",
    "\n",
    "print(img_with_padding_5x5.shape)\n",
    "plot_image(img_with_padding_5x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAP0iNC1tNqG"
   },
   "source": [
    "#### 1.1.k (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2wtYQFUYeHvu"
   },
   "outputs": [],
   "source": [
    "# TODO: use the convolved function & the sharpen filter and negative to zero to obtain a sharpened image of your\n",
    "# padded image (kernel size of 5) obtained from add_padding_to_image function \n",
    "# TODO: print the shape of the obtain sharpened image (obtained after padding)\n",
    "# TODO: plot the original image and the sharpened image (obtained after padding) side by side using the\n",
    "# plot_two_images function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaskVOCZvH__"
   },
   "source": [
    "### 1.2 Convolutional Layers (4 points)\n",
    "\n",
    "We have a 3x5x5 image (3 channels) and three 3x3x3 convolution kernels as pictured. Bias term for each feature map is also provided. For the questions below, please provide the feature/activation maps requested, please provide the python code that you used to calculate the maps.\n",
    "\n",
    "**Hint:** An image tensor should be [batch size, channels, height, weight], kernels/filters tensor should be [number of filters (output channels), filter_size_1 (input channels), filter_size_2, filter_size_3].\n",
    "\n",
    "<img src=\"https://github.com/nyumc-dl/BMSC-GA-4493-Spring2022/blob/main/Homework2/HW2_picture1.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA3xARERvIAA"
   },
   "source": [
    "What will be the dimension of the feature maps after we forward propogate the image using the given convolution kernels for the following (a) - (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z38zRex_vIAA"
   },
   "source": [
    "#### 1.2.a stride=1, padding = 0 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UD1OdPYHvIAA"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0UmBUj8vIAA"
   },
   "source": [
    "#### 1.2.b stride=2, padding = 1 (1 point) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ei_pZ03vIAA"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bEMRfxIvIAA"
   },
   "source": [
    "#### 1.2.c stride=3, padding = 2 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J91wtI02vIAB"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jcfndld1vIAB"
   },
   "source": [
    "#### 1.2.d stride=1, dilation rate=2, and padding=0 (1 point) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f19oH4EovIAB"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDYQXqPTh3AV"
   },
   "source": [
    "### 1.3 Feature Dimensions of Convolutional Neural Network (4*0.5 points)\n",
    "\n",
    "In this problem, we compute output feature shape of convolutional layers and pooling layers, which are building blocks of CNN. Let’s assume that input feature shape is C x W × H, where C is the number of channels, W is the width, and H is the height of input feature. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1E3gMDTJnNNg"
   },
   "source": [
    "\n",
    "#### 1.3.a (0.5 points)\n",
    "\n",
    "A convolutional layer has 4 hyperparameters: the filter size(K), the padding size (P), the stride step size (S) and the number of filters (F). How many weights and biases are in this convolutional layer? And what is the shape of output feature that this convolutional layer produces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuPNNA-cv_0_"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm4BUnbcnNJC"
   },
   "source": [
    "\n",
    "#### 1.3.b (0.5 points)\n",
    "\n",
    "A pooling layer has 2 hyperparameters: the stride step size(S) and the filter size (K). What is the output feature shape that this pooling layer produces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8iz3SfjrlWj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-ku40pXnUmA"
   },
   "source": [
    "\n",
    "#### 1.3.c (0.5 points)\n",
    "\n",
    "Let’s assume that we have the CNN model which consists of L successive convolutional layers and the filter size is K and the stride step size is 1 for every convolutional layer. Then what is the receptive field size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juqrKcP0w2mI"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCOKtotSnYQ3"
   },
   "source": [
    "\n",
    "#### 1.3.d (0.5 points)\n",
    "\n",
    "Consider a downsampling layer (e.g. pooling layer and strided convolution layer). In this problem, we investigate pros and cons of downsampling layer. This layer reduces the output feature resolution and this implies that the output features loose the certain amount of spatial information. Therefore when we design CNN, we usually increase the channel length to compensate this loss. For example, if we apply the max pooling layer with kernel size of 2 and stride size of 2, we increase the output feature size by a factor of 2. If we apply this max pooling layer, how much the receptive field increases? Explain the advantage of decreasing the output feature resolution with the perspective of reducing the amount of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjXNpvnHvIAB"
   },
   "source": [
    "### 1.4 (6 points)\n",
    "Use the pytorch package to calculate feature/activation maps. Write a code which takes 3x5x5 image and performs a 2D convolution operation (with stride = 1 and zero padding) using 3x3x3 filters provided on the picture. After convolution layer use leaky ReLU activation function (with negative slope 0.01) and Max-Pooling operation with required parameters to finally obtain output of dimension 3x1x1. Provide the code, feature maps obtained from convolution operation, activation maps, and feature maps after Max-Pooling operation.\n",
    "\n",
    "**Hint:** You can refer to [AdaptiveMaxPool2d](https://pytorch.org/docs/stable/nn.html#adaptivemaxpool2d) to get desired dimension output from Pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code to load image:x, kernel weights:w and bias:b\n",
    "# if you hit errors related to the long data type convert the values in your numpy arrays to floats\n",
    "import numpy as np\n",
    "import torch.nn.functional as f\n",
    "import torch\n",
    "x = np.load('q1_input.npy')\n",
    "w = np.load('q1_Filters.npy')\n",
    "b = np.load('q1_biases.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3cdMat_vIAC"
   },
   "source": [
    "### 1.5 (7 points)\n",
    "Use the pytorch package to calculate feature/activation maps of a residual unit. Example of a residual unit are seen in figure 2 of https://arxiv.org/pdf/1512.03385.pdf as well as in the figure below.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/nyumc-dl/BMSC-GA-4493-Spring2022/blob/main/Homework2/HW2_picture2.png?raw=1\" width=\"150\">\n",
    "\n",
    "Write a code which takes 3x5x5 input image and performs two 2D convolution operations using the filters provided in the figure above. Please use the three 3x3x3 filters for the two Convolution layers. You need to set a suitable padding size for the convolution operations. After the convolution layers have the residual addition and use the ReLU activation function. Provide the code and feature maps obtained from each convolution operation, activation maps, and the last activation map obtained from the residual unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nSYCGlv6vIAC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv67E0CZvIAC"
   },
   "source": [
    "### 1.6 (2 points)\n",
    "Describe the key design paramters of inception v3 (https://arxiv.org/pdf/1512.00567.pdf) and explain how it avoids overfitting of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-RP-dCpvIAC"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxadUBIwvIAC"
   },
   "source": [
    "## Question 2 Network design parameters for disease classification (Total 15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6Aghk24vIAC"
   },
   "source": [
    "Disease classification is a common problem in medicine. There are many ways to solve this problem. Goal of this question is to make sure that you have a clear picture in your mind about possible techniques that you can use in such a classification task.\n",
    "\n",
    "Assume that we have a 10K images in a dataset of computed tomography (CTs). For each image, the dimension is 16x256x256 and we have the label for each image. The label of each image defines which class the image belongs (lets assume we have 4 different disease classes in total). You will describe your approach of classifying the disease for the techniques below. Make sure you do not forget the bias term. Please provide the pytorch code which designs the network for questions 2.1.a, 2.2.a, and 2.3.a.\n",
    "\n",
    "**Hint:** See lab 4 for an example of how to make a class for a network (Implementing LeNet).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sT3i3hrCvIAD"
   },
   "outputs": [],
   "source": [
    "# starter code\n",
    "# you can generate a random image tensor for batch_size 8\n",
    "x = torch.Tensor(8,1,16,256,256).normal_().type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTsuC2IivIAD"
   },
   "source": [
    "#### 2.1.a (2 points)\n",
    "Design a multi layer perceptron (MLP) with a two hidden layer which takes an image as input (by reshaping it to a vector: let's call this a vectorized image). Our network has to first map the vectorized images to a vector of 512, then to 256 in a hidden layer and then to 128 in a hidden layer and finally feeds this vector to a fully connected layer to get the probability of 5 tissue classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BYeREVhBvIAD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGEzD-5HvIAD"
   },
   "source": [
    "#### 2.1.b (2 points)\n",
    "\n",
    "Clearly mention the sizes for your input and output at each layer until you get final output vector with 5 tissue classes and an input of images of size 16x256x256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csdp10EdvIAD"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f89xcJlFvIAD"
   },
   "source": [
    "#### 2.1.c (1 points)\n",
    "How many parameters you need to fit for your design? How does adding another hidden layer (map to 64 after 128) will effect the number of parameters to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWWzpsYwvIAD"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Mpf8B0BvIAD"
   },
   "source": [
    "#### 2.2.a (2 points)\n",
    "Design a one layer convolutional neural network which first maps the images to a vector of 256 and then 128 (both with the help of convolution and pooling operations) then feeds this vector to a fully connected layer to get the probability of 5 disease classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "tbq2N3VgvIAD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dm08jFtIvIAD"
   },
   "source": [
    "### 2.2.b (2 points)\n",
    "Clearly mention the sizes for your input, kernel, pooling, and output at each step until you get final output vector with 5 probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqBj51XBvIAD"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "319rmxntvIAD"
   },
   "source": [
    "#### 2.2.c (1 point) \n",
    "How many parameters you need to fit for your design?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rp0wnsWzvIAE"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DehnpUJqvIAE"
   },
   "source": [
    "### 2.2.d (2 points)\n",
    "Now increase your selected convolution kernel size by 4 in each direction. Describe the effect of using small vs large filter size during convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAzvORSRvIAE"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1awvrodYvIAE"
   },
   "source": [
    "### 2.3 (3 points)\n",
    "Explain your findings regading different types of neural networks and building blocks based on your observations from 2.1 and 2.2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNwXLO_UvIAE"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8VrqmJevIAE"
   },
   "source": [
    "## Question 3 Literature Review: ChestX-ray8 (Total 19 points)\n",
    "Read this paper:\n",
    "\n",
    "Pranav Rajpurkar, Jeremy Irvin, et al. \n",
    "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning https://arxiv.org/abs/1711.05225\n",
    "\n",
    "\n",
    "We are interested in understanding the goal of the task performed, the methods proposed, technical aspects of the implementation, and possible future work. After you read the full article answer the following questions. Describe your answers in your own words.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JibiHKZ-vIAE"
   },
   "source": [
    "### 3.1 (2 points) \n",
    "\n",
    "What was the underlying goal of this paper? What were the challenges in detection of pneumonia that this paper aimed at solving? What was the key motivation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1dURZE5vIAE"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hh0I37EwEtb8"
   },
   "source": [
    "### 3.2  (3 points)\n",
    "Describe the machine learning task (segmentation, classification, regression, etc?) that was attempted in this paper. Further describe the learning algorithm used (supervised, unsupervised, ..etc.) and the reason was using this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yxfx2GJMFenR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oimxPPlOvIAE"
   },
   "source": [
    "### 3.3 (2.5 points)\n",
    "How does the proposed architecture in this paper compare with the previous State of the art? Give details on the modifications and improvements, and reasons for why you think these worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3Iy_kdZvIAE"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoqL45xpvIAE"
   },
   "source": [
    "### 3.4 (2 points)\n",
    "Describe the CNN architecture used along with training details (a flow that explains the entire training process with details on the batch_size, optimizer, loss function, model weights, learning rate, etc). Also try to infer why were these paramters and hyperparamters chosen for this specific task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FutG5YyyvIAE"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UllOPc2pvIAE"
   },
   "source": [
    "### 3.5 (2.5 points)\n",
    "\n",
    "How was the model evaluated? What were the metrics utilized? List down reasons of using these metrics over all others.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhUqetEqvIAE"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IN30Y_OrvIAF"
   },
   "source": [
    "### 3.6 (2.5 points)\n",
    "\n",
    "Explain model interpretation through class activation mapping. Discuss the role of Class Activation Maps (CAMs) in CheXNet.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEv4dl6xvIAF"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4P_4LfxvIAF"
   },
   "source": [
    "### 3.7 (2 points)\n",
    "What was the kind of preprocessing the dataset went through? Explain reasons for each data transformation/preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rip3VzbgvIAF"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MmBMRexvIAF"
   },
   "source": [
    "### 3.8 (2.5 points)\n",
    "\n",
    "In the paper CAMs (class activation mappings) are used for visualisation. Can this method be used for any CNN? Describe the architectural requirements for getting CAM visualisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RrCTAjXvIAF"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8LLBSpyvIAF"
   },
   "source": [
    "## Question 4 Deep CNN design for disease classification (Total 36 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQaJ81KlvIAF"
   },
   "source": [
    "In this part of the howework, we will focus on classifiying the lung disease using chest x-ray dataset provided by NIH (https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community). You should be familiar with the dataset after answering question 3.\n",
    "\n",
    "You need to use HPC for training part of this question, as your computer's CPU will not be fast enough to compute learning iterations. Please read the HPC instruction first. In case you use HPC, please have your code/scripts uploaded under the questions and provide the required plots and tables there as well. If you run the HW2 jupter script with Squash File System and Singularity on GCP, you can find the data under /images folder. We are interested in classifying pneumothorax, cardiomegaly and infiltration cases. By saying so we have 3 classes that we want to identify by modelling a deep CNN.\n",
    "\n",
    "First, you need to work on Data_Entry_2017_v2020.csv file to identify cases/images that has infiltration, pneumothorax, and cardiomegaly. This file can be downloaded from https://nihcc.app.box.com/v/ChestXray-NIHCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOov4wcVvIAF"
   },
   "source": [
    "### 4.1 Train, Test, and Validation Sets (0.5 point)\n",
    "Write a script to read data from Data_Entry_2017.csv and process to obtain 3 sets (train, validation and test). By using 'Finding Labels' column, define a class that each image belongs to, in total you can define 3 classes:\n",
    "- 0 cardiomegaly\n",
    "- 1 pneumothorax\n",
    "- 2 infiltration\n",
    "\n",
    "Generate a train, validation and test set by splitting the whole dataset containing specific classes (0, 1, and 2)  by 70%, 10% and 20%, respectively. Test set will not be used during modelling but it will be used to test your model's accuracy. Make sure you have similar percentages of different cases in each subset. Provide statistics of the number of classess in your subsets (you do not need to think about splitting the sets based on subjects for this homework; in general, we do not want images from the same subject to appear in both train and test sets). \n",
    "\n",
    "Write a .csv files defining the samples in your train, validation and test set with names: train.csv, validation.csv, and test.csv. Submit these files with your homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39bUPWLhvIAF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOifOeMIvIAF"
   },
   "source": [
    "### 4.2 Data preparation before training (2 points)\n",
    "From here on, you will use HW2_trainSet.csv, HW2_testSet.csv and HW2_validationSet.csv provided under github repo for defining train, test and validation set samples instead of the csv files you generate on question 4.1.\n",
    "\n",
    "\n",
    "There are multiple ways of using images as an input during training or validation. Here, you will use torch Dataset class  (http://pytorch.org/tutorials/beginner/data_loading_tutorial.html). We provided an incomplete dataloader code below. Please add your code and complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQvvMXqmvIAG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io\n",
    "import torch\n",
    "from skimage import color\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"Chest X-ray dataset from https://nihcc.app.box.com/v/ChestXray-NIHCC.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file filename information.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.data_frame.iloc[idx, 0])\n",
    "\n",
    "        # TODO: read in image using io\n",
    "        # TODO: normalize the image \n",
    "        # TODO: return dictionary of image and corresponding class\n",
    "        # sample = {'x': , 'y': }\n",
    "    \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk-PqY9HvIAG"
   },
   "source": [
    "### 4.3 CNN model definition (5 points)\n",
    "Since now we can import images for model training, next step is to define a CNN model that you will use to train disease classification task. Any model requires us to select model parameters like how many layers, what is the kernel size, how many feature maps and so on. The number of possible models is infinite, but we need to make some design choices to start.  Lets design a CNN model with 4 convolutional layers, 4 residual units (similar question 1.5) and a fully connected (FC) layer followed by a classification layer. Lets use \n",
    "\n",
    "-  5x5 convolution kernels (stride 1 in resnet units and stride 2 in convolutional layers)\n",
    "-  ReLU for an activation function\n",
    "-  max pooling with kernel 2x2 and stride 2 only after the convolutional layers.\n",
    "\n",
    "Define the number of feature maps in hidden layers as: 8, 16, 32, 64, 64, 64, 128 (1st layer, ..., 7th layer). \n",
    "\n",
    "<img src=\"https://github.com/nyumc-dl/BMSC-GA-4493-Spring2022/blob/main/Homework2/HW2_picture3.png?raw=1\" height=\"300\">\n",
    "\n",
    "Write a class which specifies this network details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrQ0e_rpvIAG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1ro2KnevIAG"
   },
   "source": [
    "### 4.4 (2 point)\n",
    "How many learnable parameters of this model has? How many learnable parameters we would have if we replace the fully connected layer with global average pooling layer (Take a look at Section 3.2 of https://arxiv.org/pdf/1312.4400.pdf)?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "En_IO5vdvIAG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUp0CmTdvIAG"
   },
   "source": [
    "### 4.5 Loss function and optimizer (2 points)\n",
    "Define an appropriate loss criterion and an optimizer using pytorch. What type of loss function is applicable to our classification problem? Explain your choice of a loss function.  For an optimizer lets use Adam for now with default hyper-parmeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLgK7yThvIAG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peDaiLuuvIAG"
   },
   "source": [
    "**Some background:** In network architecture design, we want to have an architecture that has enough capacity to learn. We can achieve this by using large number of feature maps and/or many more connections and activation nodes. However, having a large number of learnable parameters can easily result in overfitting. To mitigate overfitting, we can keep the number of learnable parameters of the network small either using shallow networks or few feature maps. This approach results in underfitting that model can neither model the training data nor generalize to new data. Ideally, we want to select a model at the sweet spot between underfitting and overfitting. It is hard to find the exact sweet spot. \n",
    "\n",
    "We first need to make sure we have enough capacity to learn, without a capacity we will underfit. Here, you will need to check if designed model in 4.3 can learn or not. Since we do not need to check the generalization capacity (overfitting is OK for now since it shows learning is possible), it is a great strategy to use a subset of training samples. Also, using a subset of samples is helpful for debugging!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8nDyTCivIAG"
   },
   "source": [
    "### 4.6 Train the network on a subset (5 points)\n",
    "Lets use a script to take random samples from train set (HW2_trainSet.csv), lets name this set as HW2_randomTrainSet. Choose random samples from validation set (HW2_validationSet.csv), lets name this set as HW2_randomValidationSet. You used downsampling of images from 1024x1024 size to 64x64 in the Lab 4. This was fine for learning purpose but it will significantly reduce the infomation content of the images which is important especially in medicine. In this Homework, you MUST use original images of size 1024x1024 as the network input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMELBnHIvIAG",
    "outputId": "d53df7da-2f61-4775-cf71-c78d703c50c7"
   },
   "outputs": [],
   "source": [
    "# get samples from HW2_trainSet.csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('HW2_trainSet.csv')\n",
    "_ , X_random, _, _ = train_test_split(df, df.Class, test_size=0.1, random_state=0)\n",
    "print('Selected subset class frequencies\\n',X_random['Class'].value_counts())\n",
    "X_random.to_csv('HW2_randomTrainSet.csv',index=False)\n",
    "\n",
    "df = pd.read_csv('HW2_validationSet.csv')\n",
    "_ , X_random, _, _ = train_test_split(df, df.Class, test_size=0.1, random_state=0)\n",
    "print('Selected subset class frequencies\\n',X_random['Class'].value_counts())\n",
    "X_random.to_csv('HW2_randomValidationSet.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8L1hTzJvIAH"
   },
   "source": [
    "Use the random samples generated and write a script to train your network. Using the script train your network using your choice of weight initialization strategy. In case you need to define other hyperparameters choose them empirically, for example batch size. Plot average loss on your random sample set per epoch. (Stop the training after at most ~50 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LivnhzWivIAH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqTMs6oavIAH"
   },
   "source": [
    "### 4.7 Analysis of training using a CNN model (2 points)\n",
    "Describe your findings. Can your network learn from small subset of random samples? Does CNN model have enough capacity to learn with your choice of emprical hyperparameters?\n",
    "-  If yes, how will average loss plot will change if you multiply the learning rate by 15?\n",
    "-  If no, how can you increase the model capacity? Increase your model capacity and train again until you find a model with enough capacity. If the capacity increase is not sufficient to learn, think about empirical parameters you choose in designing your network and make some changes on your selection. Describe what type of changes you made to your original network and how can you manage this model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRIB6OcTvIAH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNqnrVyCvIAH"
   },
   "source": [
    "### 4.8 Hyperparameters (2.5 points)\n",
    "Now, we will revisit our selection of CNN model architecture, training parameters and so on: i.e. hyperparameters. In your investigations, define how you will change the hyperparameter in the light of model performance using previous hyperparameters. Provide your rationale choosing the next hyperparameter. Provide learning loss and accuracy curves, and model performance in HW2_randomValidationSet. You will use macro AUC as the performance metric for comparing CNN models for disease classification task.  Report macro AUC for each CNN model with different hyperparameters (Check http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#multiclass-settings).\n",
    "\n",
    "Investigate the effect of learning rate and batch size in the model performance (try atleast 5 learning rates and 3 batch sizes) and select optimal values for both. You only need to put your best result here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b23UDG29vIAH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLXT2SH1vIAH"
   },
   "source": [
    "### 4.9 Train the network on the whole dataset (4 points)\n",
    "After question 4.7, you should have a network which has enough capacity to learn and you were able to debug your training code so that it is now ready to be trained on the whole dataset. Use the best batch size and learning rate from 4.8. Train your network on the whole train set (HW2_trainSet_new.csv) and check the validation loss on the whole validation set (HW2_validationSet_new.csv) in each epoch. Plot average loss and accuracy on train and validation sets. Describe your findings. Do you see overfitting or underfitting to train set? What else you can do to mitigate it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCRArWixvIAH",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAMLOmFJvIAH"
   },
   "source": [
    "### 4.10 Experiments with Resnet18\n",
    "\n",
    "Let's use Resnet18 on our dataset and see how it performs. We can import the standard architectures directly using PyTorch's torchvison.models module. Refer to https://pytorch.org/docs/stable/torchvision/models.html to see all available models in PyTorch. You'll later, in this course, learn about a convenient and useful concept known as Transfer Learning. For now, we will  use the Resnet18 and train the architecture from scratch without any pre-training. Here is the link for the ResNet paper: https://arxiv.org/pdf/1512.03385.pdf ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAhsK-OCvIAH"
   },
   "source": [
    "#### 4.10.a (2 Point)\n",
    "\n",
    "What is the reason of using 1x1 convolutions before 3x3 convolutions in the resnet architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81jrnONlvIAH"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ckcsFqGvIAH"
   },
   "source": [
    "#### 4.10.b Train the ResNet18 on the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHIf3opdvIAI"
   },
   "source": [
    "We provide a new dataset class and a few additional transformations to the data for this new architecture. We have a new dataset class as ResNet18 architectures expect 3 channels in their primary input and other reasons which you'll later come to know - after the lecture on transfer learning. Nevertheless, for our case, we use them to reduce the required GPU usage as the Resnet18 architecture is significantly complex and GPU memory-intensive architecture than the CNN implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQ1v4v-VvIAI"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# torchvision models are trained on input images normalized to [0 1] range .ToPILImage() function achives this\n",
    "# additional normalization is required see: http://pytorch.org/docs/master/torchvision/models.html\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomResizedCrop(896),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.CenterCrop(896),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "class ChestXrayDataset_ResNet(Dataset):\n",
    "    \"\"\"Chest X-ray dataset from https://nihcc.app.box.com/v/ChestXray-NIHCC.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file filename information.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = load_data_and_get_class(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.data_frame.iloc[idx, 0])\n",
    "        \n",
    "        image = io.imread(img_name)\n",
    "        if len(image.shape) > 2 and image.shape[2] == 4:\n",
    "            image = image[:,:,0]\n",
    "            \n",
    "        image=np.repeat(image[None,...],3,axis=0)\n",
    "            \n",
    "        image_class = self.data_frame.iloc[idx, -1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        sample = {'x': image, 'y': image_class}\n",
    "\n",
    "        return sample\n",
    "\n",
    "def load_data_and_get_class(path_to_data):\n",
    "    data = pd.read_csv(path_to_data)\n",
    "    encoder = LabelEncoder()\n",
    "    data['Class'] = encoder.fit_transform(data['Finding Labels'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAoecd2pvIAI"
   },
   "source": [
    "#### 4.10.c Architecture modification (4.5 points) \n",
    "In this question you need to develop a CNN model based on Resnet18 architecture. Please import the original ResNet18 model from PyTorch models (You can also implement this model by your own using the resnet paper). Modify the architecture so that the model will work with full size 1024x1024 image inputs and 3 classes of our interest:\n",
    "- 0 cardiomegaly\n",
    "- 1 pneumothorax\n",
    "- 2 infiltration\n",
    "\n",
    "Make sure the model you developed uses random weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6ZuWOzwvIAI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTFrrxCevIAI"
   },
   "source": [
    "#### 4.10.d Train the network on the whole dataset (4.5 points)\n",
    "Similar to question 4.7 train the model you developed in question 4.10.b on the whole train set (HW2_trainSet_new.csv) and check the validation loss on the whole validation set (HW2_validationSet_new.csv) in each epoch. Plot average loss and accuracy on train and validation sets. Describe your findings. Do you see overfitting or underfitting to train set? What else you can do to mitigate it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vgo6rFJtvIAI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Vk-wKHxvIAI"
   },
   "source": [
    "## Question 5 Analysis of the results from two networks trained on the full dataset (Total 5 points)\n",
    "Use the validation loss to choose models from question 4.9 (model1) and question 4.10 (model2) (these models are trained on the full dataset and they learned from train data and generalized well to the validation set). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baQs7DdOvIAI"
   },
   "source": [
    "### 5.1 Model selection by performance on test set (5 Points)\n",
    "Using these models, plot confusion matrix and ROC curve for the disease classifier on the test set (HW2_TestSet_new.csv). Report AUC for this CNN model as the performance metric. You will have two confusion matrices and two ROC curves to compare model1 and model2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hFJP4qdvIAI"
   },
   "outputs": [],
   "source": [
    "# this is the place we predict the disease from a model trained, output for this function is \n",
    "# the target values and probabilty of each image having a disease \n",
    "\n",
    "# example of how to plot ROC curves\n",
    "# https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n",
    "\n",
    "# example of how to calculate confusion matrix\n",
    "# https://www.kaggle.com/grfiv4/plot-a-confusion-matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6 Bonus Questions (Maximum 12 points)\n",
    "\n",
    "**Note:** this section is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lE77scZjvIAI"
   },
   "source": [
    "### 6.1 Understanding the network (Bonus Question maximum 5 points)\n",
    "\n",
    "Even if you do both 6.1.a and 6.1.b, the max points for this question is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jofCybEfvIAI"
   },
   "source": [
    "#### 6.1.a Occlusion (5 points)\n",
    "Using the best performing model (choose the model using the analysis you performed on question 5.1), we will figure out where our network gathers infomation to decide the class for the image. One way of doing this is to occlude parts of the image and run through your network. By changing the location of the ocluded region we can visualize the probability of image being in one class as a 2-dimensional heat map. Using the best performing model, provide the heat map of the following images: HW2_visualize.csv. Do the heap map and bounding box for pathologies provide similar information? Describe your findings.\n",
    "Reference: https://arxiv.org/pdf/1311.2901.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TD4aPq5vIAI"
   },
   "outputs": [],
   "source": [
    "# you can use the code from: https://github.com/thesemicolonguy/convisualize_nb/blob/master/cnn-visualize.ipynb \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBLh8WIVvIAJ"
   },
   "source": [
    "#### 6.1.b GradCAM (5 points)\n",
    "An alternative approach to model interpretation is gradcam. Go through https://arxiv.org/pdf/1610.02391.pdf and create heatmaps of images in HW2_visualize.csv using this method. Repeat the analysis in 6.1.a and also compare the time-taken to generate occlusions and gradcams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2kVezZCvIAJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Tiling and CNNs (Bonus Question 7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using CNNs it may be helpful to first tile the image, especially for segmentation and object detection tasks. Focus on the \"Invasive Ductal Carcinoma Segmentation Use Case\" section of this [paper](https://www.sciencedirect.com/science/article/pii/S2153353922005478?via%3Dihub#tbl1). The data is avaliable [here](https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.a (0.5 points)\n",
    "\n",
    "Why is it helpful to tile an image and use the tiles as input for a CNN for segmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.b (0.5 points)\n",
    "\n",
    "Describe the hyperparameters that are introduced when you tile an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.c (0.5 points)\n",
    "\n",
    "What are some metrics that can be used to evaluate segmenation of the full image (when tiles are recombined)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.d (4 points)\n",
    "\n",
    "Load the data, train a CNN, and evaluate the performance on the dataset.\n",
    "\n",
    "**Note:** due to the size of this dataset, feel free to sample only part of the dataset to use to train and evaluate your model. Just please make sure all classes are represented, and that you do not train and test on the same patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.e (1.5 points)\n",
    "\n",
    "Select a patch of 7x7 images and predict their classification. Then display them all together as one image, and denote the patches that are predicted as IDC. Diplay another image that denotes that patches that are IDC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Q8VrqmJevIAE"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
