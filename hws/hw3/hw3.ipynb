{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Deep Learning in Medicine Spring 2025\n",
    "\n",
    "## Homework 3 - Sequence modeling\n",
    "\n",
    "Hi! This homework aims to let you get acquianted with how sequence modelling in Deep Learning can be utilized for the biology domain. \n",
    "\n",
    "We will be utilizing existing public datasets and performing 4 separate steps with Torch.\n",
    "\n",
    "Our data is related to enhancers, genomic elements which can alter the levels of gene expression.\n",
    "\n",
    "There are 4 tasks:\n",
    "\n",
    "1. RNNs/LSTMs for sequence modelling - You will be tasked to utilize existing PyTorch infrastructure and LSTMs to perform sequence classification (40 points).\n",
    "\n",
    "2. Transformers - Implement a transformer with a classification head, compare and contrast with the LSTM architecture in terms of accuracy, memory requirements, and runtime. (40 points).\n",
    "\n",
    "3. Hyperparameter optimization - Use one of your two Transformers alongside a hyperparameter optimization library to explore different hyperparameter and potentially increase performance. (30 points).\n",
    "\n",
    "4. Extracting and exploring embeddings from pre-trained SSL models - Do the big models truly understand what is an enhancer? If so, their latent representations of such sequences should be meaningful. (30 points).\n",
    "\n",
    "The maximum you can get for the whole homework is 140. This means that you can choose to solve two assignments fully and one partially depending on your interests and/or familiarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you install the [https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks](genomic-benchmarks) package with ```pip install genomic-benchmarks```. Make sure your gdown package is updated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/nikolas/Desktop/Projects/TA/BMSC-GA-4493-Spring2025/hws/hw3'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from genomic_benchmarks.data_check import list_datasets\n",
    "from genomic_benchmarks.data_check import info\n",
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "\n",
    "info('human_enhancers_ensembl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the dataset is quite large, so make sure you have GPU access when using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " download_dataset(\"human_enhancers_ensembl\") #51MB zipped, 600 MB full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset\n",
    "\n",
    "Luckily the repository gives us ample instructions on how to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanEnhancersEnsembl\n",
    "train = HumanEnhancersEnsembl(split='train', version=0)\n",
    "test = HumanEnhancersEnsembl(split='test', version=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset exploration (5 points)\n",
    "\n",
    "Time to take a look at the data!\n",
    "\n",
    "Use your code to check:\n",
    "\n",
    "1. Number of examples in train and test datasets (A simple print will do)\n",
    "2. Structure of examples (You can use)\n",
    "3. Maximum of sequences (Important padding length!)\n",
    "4. Number of nucleotides, di-nucleotides, trinucleotides (Tip: You can use defaultdict!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of train examples:\", len(train))\n",
    "print(\"Number of test examples:\", len(test))\n",
    "\n",
    "print(\"Structure of a training example:\", train[0])\n",
    "\n",
    "import collections\n",
    "max_len = 0\n",
    "nuc_counts = collections.defaultdict(int)\n",
    "dinuc_counts = collections.defaultdict(int)\n",
    "trinuc_counts = collections.defaultdict(int)\n",
    "\n",
    "for sample in train:\n",
    "    seq = sample[0] \n",
    "    max_len = max(max_len, len(seq))\n",
    "    for char in seq:\n",
    "        nuc_counts[char] += 1\n",
    "    for i in range(len(seq) - 1):\n",
    "        dinuc_counts[seq[i:i+2]] += 1\n",
    "    for i in range(len(seq) - 2):\n",
    "        trinuc_counts[seq[i:i+3]] += 1\n",
    "\n",
    "print(\"Maximum sequence length (train):\", max_len)\n",
    "print(\"Nucleotide counts:\", dict(nuc_counts))\n",
    "print(\"Di-nucleotide counts:\", dict(dinuc_counts))\n",
    "print(\"Tri-nucleotide counts:\", dict(trinuc_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a separate dataset with non-overlapping n-nucleotides (5 points)\n",
    "\n",
    "We will use those to test the difference of utilizing different levels of encoding our bases (by 1, 2, 3).\n",
    "\n",
    "Make sure to generate the vocabulary and any special tokens you might need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:\n",
    "def generate_n_nucleotides_dataset(train, n):\n",
    "    \"\"\"\n",
    "    Generate a dataset with non-overlapping n-nucleotides and apply padding.\n",
    "    Args:\n",
    "        train: Original training dataset.\n",
    "        n: Length of n-nucleotides.\n",
    "    Returns:\n",
    "        transformed_dataset: List of transformed sequences with n-nucleotides.\n",
    "        vocab: Vocabulary of n-nucleotides.\n",
    "    \"\"\"\n",
    "    import collections\n",
    "\n",
    "    transformed_dataset = []\n",
    "    vocab = set()\n",
    "    special_tokens = {\"<PAD>\", \"<UNK>\"}\n",
    "    vocab.update(special_tokens)\n",
    "\n",
    "    max_seq_len = 0  # Track the maximum sequence length for padding\n",
    "\n",
    "    for sample in train:\n",
    "        seq = sample[0]\n",
    "        n_nucleotides = [seq[i:i+n] for i in range(0, len(seq), n) if len(seq[i:i+n]) == n]\n",
    "        max_seq_len = max(max_seq_len, len(n_nucleotides))  # Update max sequence length\n",
    "        transformed_dataset.append((n_nucleotides, sample[1]))\n",
    "        vocab.update(n_nucleotides)\n",
    "\n",
    "    # Apply padding to sequences\n",
    "    for i in range(len(transformed_dataset)):\n",
    "        n_nucleotides, label = transformed_dataset[i]\n",
    "        padded_sequence = n_nucleotides + [\"<PAD>\"] * (max_seq_len - len(n_nucleotides))\n",
    "        transformed_dataset[i] = (padded_sequence, label)\n",
    "\n",
    "    vocab = sorted(vocab)  # Sort for consistency\n",
    "    return transformed_dataset, vocab\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NucleotideDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for n-nucleotide sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, transformed_dataset, vocab):\n",
    "        self.data = transformed_dataset\n",
    "        self.vocab = {token: idx for idx, token in enumerate(vocab)}\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, label = self.data[idx]\n",
    "        encoded_sequence = [self.vocab.get(token, self.vocab[self.unk_token]) for token in sequence]\n",
    "        return torch.tensor(encoded_sequence, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "n = 3  # Change this to 1, 2, or 3 as needed\n",
    "transformed_train, vocab = generate_n_nucleotides_dataset(train, n)\n",
    "transformed_test, vocab = generate_n_nucleotides_dataset(train, n)\n",
    "train_dataset = NucleotideDataset(transformed_train, vocab)\n",
    "test_dataset = NucleotideDataset(transformed_test, vocab)\n",
    "\n",
    "# Example DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, drop_last=False)\n",
    "# Iterate through the DataLoader\n",
    "for batch in train_loader:\n",
    "    sequences, labels = batch\n",
    "    print(\"Batch sequences:\", sequences)\n",
    "    print(\"Batch labels:\", labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a PyTorch LSTM single-layer LSTM model to classify sequences (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the LSTMClassifier class\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, vocab_size, pad_idx):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, input_size, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = self.fc(hidden[-1])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Define training and testing loops\n",
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for sequences, labels in train_loader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(train_loader), accuracy\n",
    "\n",
    "# Define testing loop\n",
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(test_loader), accuracy\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model parameters\n",
    "input_size = 128  # Embedding size\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "num_classes = len(set([label for _, label in transformed_train]))  # Number of unique labels\n",
    "vocab_size = len(vocab)\n",
    "pad_idx = vocab.index(\"<PAD>\")\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes, vocab_size, pad_idx).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = cross_entropy\n",
    "\n",
    "# Training and testing the model\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_model(model, train_bloader, optimizer, criterion, device)\n",
    "    test_loss, test_accuracy = test_model(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate AUC on the testing set for your model and plot the loss curve (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat this exercise using an n-nucleotide representation of your choosing. Compare and contrast the accuracy, AUC, stability, training time, memory requirements, and inference time of your model (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Transformer (40 points)\n",
    "\n",
    "As you all know, the Transformer architecture has taken the world of Natural Language Processing by storm and the world of biology followed soon after. The transformer is the backbone upon which most of the necessary architectures are built nowadays.\n",
    "\n",
    "In this part of the assignment, you are asked to build an initial transformer and iterate on it, through linearization of attention to reduce requirements and runtime, incredible engineering feats such as flash-attention as well as read through and implement some of the more academic versions through outside resources such as x-transformers and huggingface's libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a vanilla transformer on a subset of the data (10 points)\n",
    "\n",
    "You can use 1/10th, or 1/5th of the data depending on runtimes.\n",
    "\n",
    "Evaluate the results on the test set.\n",
    "\n",
    "Never forget your positional encodings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a linearized transformer (your choice) on a subset of the data (10 points)\n",
    "\n",
    "Use a linearized transformer and re-train. Juxtapose differences in memory and runtime requirements requirements, accuracy, AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a transformer that uses Flash Attention (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a model that uses recent architectural improvements in transformers (10 points)\n",
    "\n",
    "A great reference is here: https://github.com/lucidrains/x-transformers. You're free to use any combination of architecture and optimizer you would like to test. E.g. layers of BatchNorm, multiple transformer layers, MLA implementations, second-order optimizers, torch JIT compilation, etc. \n",
    "\n",
    "Please write a small paragraph about why you chose that specific architectural development. You are not required to use xtransformers, but please point me to the library you utilized and steps to install it.\n",
    "\n",
    "Once again, compare and constrast for runtime, memory, accuracy, AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization (30 points)\n",
    "\n",
    "Sometimes you should be willing to give up some accuracy for portability (requirements - ease of running on consumer-grade hardware or \"the edge\") and usability (inference time - e.g. in case this would be running as a service).\n",
    "\n",
    "In this part of the assignments, use a hyperparameter optimization library (preferably optuna) and the subset of the training data to find the best hyperparameters for either the LSTM or the transformer you implemented in the the previous parts of the assignment.\n",
    "\n",
    "In this part of the assignment, you are aiming to find the best trade-off of accuracy, runtime, and requirements.\n",
    "\n",
    "You are expected to manipulate the following hyperparameters (not exhaustively):\n",
    "\n",
    "- Learning rate\n",
    "- Number of layers/heads\n",
    "- Hidden states\n",
    "- Embedding size\n",
    "- Dropout rate\n",
    "- Batch normalization\n",
    "\n",
    "After you have found the best hyperparameters, train the model on the full training dataset and report accuracy, AUC and running time, both for training.\n",
    "\n",
    "Submissions will be judged based on the following criteria:\n",
    "\n",
    "- Correctness of code (20 points)\n",
    "- Training/Inference time and requirements (5 points)\n",
    "- Total accuracy (5 points)\n",
    "\n",
    "The latter two will be judged across the whole class.\n",
    "\n",
    "All code will be tested on Tesla V100s (16GB) - Please do not use larger GPUs for your training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizing SSL models (30 points)\n",
    "\n",
    "Many times in daily research, you are not required to train these huge models on massive corpa from scratch. Institutions, as well as companies, have been generating models, and data and training them. You can leverage these models to extract useful embeddings for downstream tasks.\n",
    "\n",
    "In this part of the assignment, you are asked to utilize Evo, a foundation DNA language model to extract embeddings of your positive and negative sequences and then embark on a journey of data exploration.\n",
    "\n",
    "The original Evo model can be found here: https://github.com/evo-design/evo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the evo package and successfully run the following example code (5 points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo import Evo\n",
    "import torch\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "evo_model = Evo('evo-1-131k-base')\n",
    "model, tokenizer = evo_model.model, evo_model.tokenizer\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "sequence = 'ACGT'\n",
    "input_ids = torch.tensor(\n",
    "    tokenizer.tokenize(sequence),\n",
    "    dtype=torch.int,\n",
    ").to(device).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, _ = model(input_ids) # (batch, length, vocab)\n",
    "\n",
    "print('Logits: ', logits)\n",
    "print('Shape (batch, length, vocab): ', logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings from the model utilizing the subset of your sequences (5 points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory analysis (15 points):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a dimensionality reduction of your embeddings, colored by their label. You can use PCA/tSNE/UMAP. Visualize it and comment on cluster segregation. Additionally, for each of your sequences, color each sequence by their dominant n-nucleotide, if that n-nucleotide's counts accounts for above 0.5/n of all dinucleotides. (e.g. if you're doing dinucleotides, you use 0.25, trinucleotides you use 0.5/0.3, tetranucleotides you use 0.125, pentanucleotides 0.1 etc.). Comment on any interesting findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize a clustering algorithm (e.g. K-means, Leiden, H-DBSCAN) to cluster the embeddings. Is the clustering reflected on the UMAP? Plot the proportions of your chosen n-nucleotide in each of the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on top of the embeddings (10 points):\n",
    "\n",
    "Did we even need the pre-trained transformer at all?\n",
    "\n",
    "Training a multi-layer MLP on top of the embeddings to classify enhancer from non-enhancer sequences. How does it compare to the models you already trained based on accuracy, AUC, runtime and memory requirements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code goes here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
